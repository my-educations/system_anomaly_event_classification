{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation, metrics\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.4)\n",
    "sess_config = tf.ConfigProto(gpu_options=gpu_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = {'0':'file', '1':'network', '2':'service', '3':'database', '4':'communication', '5':'memory', '6':'driver', \n",
    "    '7':'system', '8':'application', '9':'io', '10':'others', '11':'security', '12':'disk', '13':'processor'}\n",
    "\n",
    "fault_label = {'0':'file', '1':'network', '2':'service', '3':'database', '4':'communication', '5':'memory', '6':'driver', \n",
    "    '7':'system','9':'io', '10':'others', '11':'security', '12':'disk', '13':'processor'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset separated.\n",
      "\n",
      "((80000, 10, 14, 1), (80000, 14), (20000, 10, 14, 1), (20000, 14))\n"
     ]
    }
   ],
   "source": [
    "def one_hot(y):\n",
    "    y = y.reshape(len(y))\n",
    "    n_values = np.max(y) + 1\n",
    "    return np.eye(n_values)[np.array(y, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "\n",
    "def load_X(X_path):\n",
    "    X_list = []\n",
    "    file = open(X_path, 'r')\n",
    "    # Read dataset from disk, dealing with text files' syntax\n",
    "    X_signal = [np.array(item, dtype=np.float32) for item in [\n",
    "               line.strip().split('\\t') for line in file]]\n",
    "    X_list.append(X_signal)\n",
    "    file.close()\n",
    "    return np.transpose(np.array(X_list), (1, 2, 0))\n",
    "\n",
    "\n",
    "# Load \"y\" (the neural network's training and testing outputs)\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array([elem for elem in [line.strip().split('\\t') for line in file]], \n",
    "                  dtype=np.int32)\n",
    "    file.close()\n",
    "    # Substract 1 to each output class for friendly 0-based indexing\n",
    "    return one_hot(y_-1)\n",
    "\n",
    "\n",
    "dataset_path = \"../data_msg_type/\"\n",
    "trainX_path = dataset_path + \"semantic_train_x.txt\"\n",
    "trainY_path = dataset_path + \"semantic_train_y.txt\"\n",
    "testX_path = dataset_path + \"semantic_test_x.txt\"\n",
    "testY_path = dataset_path + \"semantic_test_y.txt\"\n",
    "\n",
    "\n",
    "train_x = load_X(trainX_path)\n",
    "train_y = load_y(trainY_path)\n",
    "test_x = load_X(testX_path)\n",
    "test_y = load_y(testY_path)\n",
    "\n",
    "train_x = train_x.reshape(len(train_x), 10, 14, 1)\n",
    "test_x = test_x.reshape(len(test_x), 10, 14, 1)\n",
    "\n",
    "'''\n",
    "# Separate our training data into test and training.\n",
    "print(\"Separating data into 80% training set & 20% test set...\")\n",
    "train_x, test_x, train_y, test_y = cross_validation.train_test_split(\n",
    "    x, y, test_size=0.2, random_state=33)#add random state here...\n",
    "'''\n",
    "\n",
    "print(\"Dataset separated.\\n\")\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "training_iters = 16000000\n",
    "batch_size = 1000\n",
    "display_step = 20000\n",
    "\n",
    "# Network Parameters\n",
    "input_height = 10\n",
    "input_width = 14\n",
    "num_channels = 1\n",
    "n_classes = 14\n",
    "dropout = 0.75 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 10, 14, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    print(conv1.shape)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    print(conv2.shape)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
    "    conv4 = conv2d(conv3, weights['wc4'], biases['bc4'])\n",
    "    conv5 = conv2d(conv4, weights['wc5'], biases['bc5'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv5 = maxpool2d(conv5, k=2)\n",
    "    print(conv3.shape)\n",
    "    print(conv4.shape)\n",
    "    print(conv5.shape)\n",
    "\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv5, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    \n",
    "    # Fully connected layer\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    print(fc1.shape)\n",
    "\n",
    "    # Output, class prediction\n",
    "    fc2 = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    print(fc2.shape)\n",
    "    return fc2\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 3x4 conv, 1 input, 16 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([3, 4, 1, 16])),\n",
    "    # 3x4 conv, 32 inputs, 16 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([3, 4, 16, 16])),\n",
    "    # 3x4 conv, 64 inputs, 16 outputs\n",
    "    'wc3': tf.Variable(tf.random_normal([3, 4, 16, 16])),\n",
    "    # 3x4 conv, 64 inputs, 16 outputs\n",
    "    'wc4': tf.Variable(tf.random_normal([3, 4, 16, 16])),\n",
    "    # 3x4 conv, 32 inputs, 16 outputs\n",
    "    'wc5': tf.Variable(tf.random_normal([3, 4, 16, 16])),\n",
    "    # fully connected, 2*2*16 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([2*2*16, 1024])),\n",
    "    # 1024 inputs, 14 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([16])),\n",
    "    'bc2': tf.Variable(tf.random_normal([16])),\n",
    "    'bc3': tf.Variable(tf.random_normal([16])),\n",
    "    'bc4': tf.Variable(tf.random_normal([16])),\n",
    "    'bc5': tf.Variable(tf.random_normal([16])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 5, 7, 16)\n",
      "(?, 3, 4, 16)\n",
      "(?, 3, 4, 16)\n",
      "(?, 3, 4, 16)\n",
      "(?, 2, 2, 16)\n",
      "(?, 1024)\n",
      "(?, 14)\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None,input_height,input_width,num_channels])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #1000: Batch Loss = 3122381.000000, Accuracy = 0.0399999991059\n",
      "Performance on test set: Training epochs #1000, Batch Loss = 1215405.0, Accuracy = 0.131999999285\n",
      "Training epochs #20000: Batch Loss = 3892367.750000, Accuracy = 0.0170000009239\n",
      "Performance on test set: Training epochs #20000, Batch Loss = 1252224.625, Accuracy = 0.00600000005215\n",
      "Training epochs #40000: Batch Loss = 1488805.875000, Accuracy = 0.042999997735\n",
      "Performance on test set: Training epochs #40000, Batch Loss = 926801.5, Accuracy = 0.00600000005215\n",
      "Training epochs #60000: Batch Loss = 1111177.000000, Accuracy = 0.115000002086\n",
      "Performance on test set: Training epochs #60000, Batch Loss = 440791.25, Accuracy = 0.00600000005215\n",
      "Training epochs #80000: Batch Loss = 764924.250000, Accuracy = 0.231999993324\n",
      "Performance on test set: Training epochs #80000, Batch Loss = 156422.203125, Accuracy = 0.805000007153\n",
      "Training epochs #100000: Batch Loss = 3044637.750000, Accuracy = 0.0190000012517\n",
      "Performance on test set: Training epochs #100000, Batch Loss = 152496.671875, Accuracy = 0.805000007153\n",
      "Training epochs #120000: Batch Loss = 603357.812500, Accuracy = 0.300999999046\n",
      "Performance on test set: Training epochs #120000, Batch Loss = 161125.15625, Accuracy = 0.804999947548\n",
      "Training epochs #140000: Batch Loss = 518548.093750, Accuracy = 0.377999991179\n",
      "Performance on test set: Training epochs #140000, Batch Loss = 189966.390625, Accuracy = 0.805000066757\n",
      "Training epochs #160000: Batch Loss = 366580.250000, Accuracy = 0.527999997139\n",
      "Performance on test set: Training epochs #160000, Batch Loss = 210284.71875, Accuracy = 0.805000007153\n",
      "Training epochs #180000: Batch Loss = 2639651.500000, Accuracy = 0.0340000018477\n",
      "Performance on test set: Training epochs #180000, Batch Loss = 169859.453125, Accuracy = 0.804999947548\n",
      "Training epochs #200000: Batch Loss = 373334.781250, Accuracy = 0.423999994993\n",
      "Performance on test set: Training epochs #200000, Batch Loss = 158752.34375, Accuracy = 0.805000066757\n",
      "Training epochs #220000: Batch Loss = 373786.625000, Accuracy = 0.511999964714\n",
      "Performance on test set: Training epochs #220000, Batch Loss = 174130.1875, Accuracy = 0.805000007153\n",
      "Training epochs #240000: Batch Loss = 286590.093750, Accuracy = 0.550000011921\n",
      "Performance on test set: Training epochs #240000, Batch Loss = 182101.25, Accuracy = 0.805000007153\n",
      "Training epochs #260000: Batch Loss = 2200654.250000, Accuracy = 0.062000002712\n",
      "Performance on test set: Training epochs #260000, Batch Loss = 138153.71875, Accuracy = 0.809000015259\n",
      "Training epochs #280000: Batch Loss = 329267.187500, Accuracy = 0.458000004292\n",
      "Performance on test set: Training epochs #280000, Batch Loss = 127997.867188, Accuracy = 0.809000015259\n",
      "Training epochs #300000: Batch Loss = 286249.875000, Accuracy = 0.56400001049\n",
      "Performance on test set: Training epochs #300000, Batch Loss = 140660.046875, Accuracy = 0.805000066757\n",
      "Training epochs #320000: Batch Loss = 222445.390625, Accuracy = 0.627000033855\n",
      "Performance on test set: Training epochs #320000, Batch Loss = 146528.03125, Accuracy = 0.805000066757\n",
      "Training epochs #340000: Batch Loss = 1834835.250000, Accuracy = 0.0689999982715\n",
      "Performance on test set: Training epochs #340000, Batch Loss = 107298.476562, Accuracy = 0.809000015259\n",
      "Training epochs #360000: Batch Loss = 258411.968750, Accuracy = 0.513000011444\n",
      "Performance on test set: Training epochs #360000, Batch Loss = 99047.4609375, Accuracy = 0.809000015259\n",
      "Training epochs #380000: Batch Loss = 249081.218750, Accuracy = 0.60799998045\n",
      "Performance on test set: Training epochs #380000, Batch Loss = 111767.3125, Accuracy = 0.809000074863\n",
      "Training epochs #400000: Batch Loss = 176384.437500, Accuracy = 0.664999961853\n",
      "Performance on test set: Training epochs #400000, Batch Loss = 116828.03125, Accuracy = 0.809000015259\n",
      "Training epochs #420000: Batch Loss = 1542600.750000, Accuracy = 0.070999994874\n",
      "Performance on test set: Training epochs #420000, Batch Loss = 82904.0859375, Accuracy = 0.809000074863\n",
      "Training epochs #440000: Batch Loss = 222505.406250, Accuracy = 0.541999936104\n",
      "Performance on test set: Training epochs #440000, Batch Loss = 76302.21875, Accuracy = 0.809000015259\n",
      "Training epochs #460000: Batch Loss = 200288.875000, Accuracy = 0.625\n",
      "Performance on test set: Training epochs #460000, Batch Loss = 88718.09375, Accuracy = 0.809000015259\n",
      "Training epochs #480000: Batch Loss = 150543.468750, Accuracy = 0.67300003767\n",
      "Performance on test set: Training epochs #480000, Batch Loss = 92653.0234375, Accuracy = 0.809000015259\n",
      "Training epochs #500000: Batch Loss = 1282238.875000, Accuracy = 0.0810000002384\n",
      "Performance on test set: Training epochs #500000, Batch Loss = 59274.8515625, Accuracy = 0.808999955654\n",
      "Training epochs #520000: Batch Loss = 195780.546875, Accuracy = 0.528999984264\n",
      "Performance on test set: Training epochs #520000, Batch Loss = 55323.46875, Accuracy = 0.809000015259\n",
      "Training epochs #540000: Batch Loss = 181662.375000, Accuracy = 0.623000025749\n",
      "Performance on test set: Training epochs #540000, Batch Loss = 69249.6484375, Accuracy = 0.809000015259\n",
      "Training epochs #560000: Batch Loss = 146373.671875, Accuracy = 0.674000024796\n",
      "Performance on test set: Training epochs #560000, Batch Loss = 73709.703125, Accuracy = 0.809000074863\n",
      "Training epochs #580000: Batch Loss = 1103358.750000, Accuracy = 0.0950000062585\n",
      "Performance on test set: Training epochs #580000, Batch Loss = 48103.5390625, Accuracy = 0.943000018597\n",
      "Training epochs #600000: Batch Loss = 169275.718750, Accuracy = 0.560000002384\n",
      "Performance on test set: Training epochs #600000, Batch Loss = 44362.1015625, Accuracy = 0.943000018597\n",
      "Training epochs #620000: Batch Loss = 137275.234375, Accuracy = 0.68799996376\n",
      "Performance on test set: Training epochs #620000, Batch Loss = 52496.984375, Accuracy = 0.809000015259\n",
      "Training epochs #640000: Batch Loss = 116901.992188, Accuracy = 0.708999991417\n",
      "Performance on test set: Training epochs #640000, Batch Loss = 55730.7421875, Accuracy = 0.809000015259\n",
      "Training epochs #660000: Batch Loss = 938534.250000, Accuracy = 0.0949999988079\n",
      "Performance on test set: Training epochs #660000, Batch Loss = 40769.484375, Accuracy = 0.943000078201\n",
      "Training epochs #680000: Batch Loss = 138899.750000, Accuracy = 0.585999965668\n",
      "Performance on test set: Training epochs #680000, Batch Loss = 37553.8476562, Accuracy = 0.943000078201\n",
      "Training epochs #700000: Batch Loss = 119910.273438, Accuracy = 0.672999978065\n",
      "Performance on test set: Training epochs #700000, Batch Loss = 43685.4257812, Accuracy = 0.942999958992\n",
      "Training epochs #720000: Batch Loss = 95833.320312, Accuracy = 0.735000014305\n",
      "Performance on test set: Training epochs #720000, Batch Loss = 46647.2734375, Accuracy = 0.942999958992\n",
      "Training epochs #740000: Batch Loss = 791321.500000, Accuracy = 0.120000004768\n",
      "Performance on test set: Training epochs #740000, Batch Loss = 34305.2304688, Accuracy = 0.943000078201\n",
      "Training epochs #760000: Batch Loss = 129067.351562, Accuracy = 0.592000007629\n",
      "Performance on test set: Training epochs #760000, Batch Loss = 31463.0058594, Accuracy = 0.943000018597\n",
      "Training epochs #780000: Batch Loss = 118064.976562, Accuracy = 0.661000013351\n",
      "Performance on test set: Training epochs #780000, Batch Loss = 37248.4375, Accuracy = 0.943000018597\n",
      "Training epochs #800000: Batch Loss = 92548.359375, Accuracy = 0.704999983311\n",
      "Performance on test set: Training epochs #800000, Batch Loss = 40213.203125, Accuracy = 0.943000078201\n",
      "Training epochs #820000: Batch Loss = 677240.750000, Accuracy = 0.119000002742\n",
      "Performance on test set: Training epochs #820000, Batch Loss = 29041.4628906, Accuracy = 0.943000078201\n",
      "Training epochs #840000: Batch Loss = 115591.625000, Accuracy = 0.622000038624\n",
      "Performance on test set: Training epochs #840000, Batch Loss = 26957.4570312, Accuracy = 0.943000078201\n",
      "Training epochs #860000: Batch Loss = 95936.859375, Accuracy = 0.68900001049\n",
      "Performance on test set: Training epochs #860000, Batch Loss = 32695.4648438, Accuracy = 0.942999958992\n",
      "Training epochs #880000: Batch Loss = 73317.835938, Accuracy = 0.738000094891\n",
      "Performance on test set: Training epochs #880000, Batch Loss = 35270.7421875, Accuracy = 0.943000018597\n",
      "Training epochs #900000: Batch Loss = 566794.187500, Accuracy = 0.131999999285\n",
      "Performance on test set: Training epochs #900000, Batch Loss = 24925.2792969, Accuracy = 0.942999958992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #920000: Batch Loss = 105406.062500, Accuracy = 0.615999996662\n",
      "Performance on test set: Training epochs #920000, Batch Loss = 23120.9140625, Accuracy = 0.943000078201\n",
      "Training epochs #940000: Batch Loss = 75156.679688, Accuracy = 0.724000036716\n",
      "Performance on test set: Training epochs #940000, Batch Loss = 28703.7226562, Accuracy = 0.942999958992\n",
      "Training epochs #960000: Batch Loss = 64273.726562, Accuracy = 0.759000062943\n",
      "Performance on test set: Training epochs #960000, Batch Loss = 31236.921875, Accuracy = 0.942999958992\n",
      "Training epochs #980000: Batch Loss = 498495.906250, Accuracy = 0.153999999166\n",
      "Performance on test set: Training epochs #980000, Batch Loss = 21538.5390625, Accuracy = 0.943000078201\n",
      "Training epochs #1000000: Batch Loss = 95077.648438, Accuracy = 0.627999961376\n",
      "Performance on test set: Training epochs #1000000, Batch Loss = 20482.0175781, Accuracy = 0.943000078201\n",
      "Training epochs #1020000: Batch Loss = 76240.234375, Accuracy = 0.708000004292\n",
      "Performance on test set: Training epochs #1020000, Batch Loss = 25880.296875, Accuracy = 0.942999958992\n",
      "Training epochs #1040000: Batch Loss = 55875.046875, Accuracy = 0.770999968052\n",
      "Performance on test set: Training epochs #1040000, Batch Loss = 28327.9296875, Accuracy = 0.943000078201\n",
      "Training epochs #1060000: Batch Loss = 444765.500000, Accuracy = 0.165000006557\n",
      "Performance on test set: Training epochs #1060000, Batch Loss = 18689.7734375, Accuracy = 0.942999958992\n",
      "Training epochs #1080000: Batch Loss = 70588.023438, Accuracy = 0.655000030994\n",
      "Performance on test set: Training epochs #1080000, Batch Loss = 17748.2480469, Accuracy = 0.943000078201\n",
      "Training epochs #1100000: Batch Loss = 67002.640625, Accuracy = 0.740999996662\n",
      "Performance on test set: Training epochs #1100000, Batch Loss = 23359.4628906, Accuracy = 0.943000078201\n",
      "Training epochs #1120000: Batch Loss = 47333.046875, Accuracy = 0.785000026226\n",
      "Performance on test set: Training epochs #1120000, Batch Loss = 25850.5390625, Accuracy = 0.943000018597\n",
      "Training epochs #1140000: Batch Loss = 399824.375000, Accuracy = 0.171000003815\n",
      "Performance on test set: Training epochs #1140000, Batch Loss = 16520.0566406, Accuracy = 0.943000078201\n",
      "Training epochs #1160000: Batch Loss = 85863.335938, Accuracy = 0.615999996662\n",
      "Performance on test set: Training epochs #1160000, Batch Loss = 15923.0390625, Accuracy = 0.942999958992\n",
      "Training epochs #1180000: Batch Loss = 57998.804688, Accuracy = 0.729000031948\n",
      "Performance on test set: Training epochs #1180000, Batch Loss = 21197.7226562, Accuracy = 0.943000078201\n",
      "Training epochs #1200000: Batch Loss = 45738.589844, Accuracy = 0.795000016689\n",
      "Performance on test set: Training epochs #1200000, Batch Loss = 23535.4648438, Accuracy = 0.943000078201\n",
      "Training epochs #1220000: Batch Loss = 359970.562500, Accuracy = 0.183999985456\n",
      "Performance on test set: Training epochs #1220000, Batch Loss = 14942.6445312, Accuracy = 0.943000018597\n",
      "Training epochs #1240000: Batch Loss = 64702.601562, Accuracy = 0.661000013351\n",
      "Performance on test set: Training epochs #1240000, Batch Loss = 14516.3085938, Accuracy = 0.943000078201\n",
      "Training epochs #1260000: Batch Loss = 62989.890625, Accuracy = 0.731000006199\n",
      "Performance on test set: Training epochs #1260000, Batch Loss = 19670.5898438, Accuracy = 0.943000018597\n",
      "Training epochs #1280000: Batch Loss = 40049.921875, Accuracy = 0.800999999046\n",
      "Performance on test set: Training epochs #1280000, Batch Loss = 21773.6367188, Accuracy = 0.943000078201\n",
      "Training epochs #1300000: Batch Loss = 320892.375000, Accuracy = 0.199000000954\n",
      "Performance on test set: Training epochs #1300000, Batch Loss = 13862.1933594, Accuracy = 0.943000078201\n",
      "Training epochs #1320000: Batch Loss = 67203.515625, Accuracy = 0.660000085831\n",
      "Performance on test set: Training epochs #1320000, Batch Loss = 13365.3691406, Accuracy = 0.943000018597\n",
      "Training epochs #1340000: Batch Loss = 51693.988281, Accuracy = 0.740000009537\n",
      "Performance on test set: Training epochs #1340000, Batch Loss = 18055.0683594, Accuracy = 0.943000078201\n",
      "Training epochs #1360000: Batch Loss = 37738.820312, Accuracy = 0.800999999046\n",
      "Performance on test set: Training epochs #1360000, Batch Loss = 20120.609375, Accuracy = 0.943000078201\n",
      "Training epochs #1380000: Batch Loss = 336188.437500, Accuracy = 0.184000000358\n",
      "Performance on test set: Training epochs #1380000, Batch Loss = 12623.3583984, Accuracy = 0.942999958992\n",
      "Training epochs #1400000: Batch Loss = 61590.894531, Accuracy = 0.664999961853\n",
      "Performance on test set: Training epochs #1400000, Batch Loss = 12424.9970703, Accuracy = 0.943000078201\n",
      "Training epochs #1420000: Batch Loss = 45532.488281, Accuracy = 0.75100004673\n",
      "Performance on test set: Training epochs #1420000, Batch Loss = 16864.6796875, Accuracy = 0.943000078201\n",
      "Training epochs #1440000: Batch Loss = 38436.597656, Accuracy = 0.799000024796\n",
      "Performance on test set: Training epochs #1440000, Batch Loss = 18704.9101562, Accuracy = 0.943000078201\n",
      "Training epochs #1460000: Batch Loss = 304455.937500, Accuracy = 0.191000014544\n",
      "Performance on test set: Training epochs #1460000, Batch Loss = 11900.3955078, Accuracy = 0.943000078201\n",
      "Training epochs #1480000: Batch Loss = 53501.308594, Accuracy = 0.695000052452\n",
      "Performance on test set: Training epochs #1480000, Batch Loss = 11526.1386719, Accuracy = 0.943000018597\n",
      "Training epochs #1500000: Batch Loss = 48717.003906, Accuracy = 0.760999977589\n",
      "Performance on test set: Training epochs #1500000, Batch Loss = 15422.3994141, Accuracy = 0.942999958992\n",
      "Training epochs #1520000: Batch Loss = 36391.117188, Accuracy = 0.806000053883\n",
      "Performance on test set: Training epochs #1520000, Batch Loss = 17061.953125, Accuracy = 0.943000018597\n",
      "Training epochs #1540000: Batch Loss = 267019.625000, Accuracy = 0.228000000119\n",
      "Performance on test set: Training epochs #1540000, Batch Loss = 10983.0292969, Accuracy = 0.942999958992\n",
      "Training epochs #1560000: Batch Loss = 52182.105469, Accuracy = 0.690999984741\n",
      "Performance on test set: Training epochs #1560000, Batch Loss = 10758.7255859, Accuracy = 0.943000078201\n",
      "Training epochs #1580000: Batch Loss = 40623.109375, Accuracy = 0.771999955177\n",
      "Performance on test set: Training epochs #1580000, Batch Loss = 14213.9882812, Accuracy = 0.942999958992\n",
      "Training epochs #1600000: Batch Loss = 33494.308594, Accuracy = 0.816999971867\n",
      "Performance on test set: Training epochs #1600000, Batch Loss = 15713.4160156, Accuracy = 0.943000078201\n",
      "Training epochs #1620000: Batch Loss = 254974.062500, Accuracy = 0.237999990582\n",
      "Performance on test set: Training epochs #1620000, Batch Loss = 10338.6210938, Accuracy = 0.943000078201\n",
      "Training epochs #1640000: Batch Loss = 46741.964844, Accuracy = 0.691999971867\n",
      "Performance on test set: Training epochs #1640000, Batch Loss = 10038.0966797, Accuracy = 0.942999958992\n",
      "Training epochs #1660000: Batch Loss = 41851.039062, Accuracy = 0.764999985695\n",
      "Performance on test set: Training epochs #1660000, Batch Loss = 13143.8710938, Accuracy = 0.943000078201\n",
      "Training epochs #1680000: Batch Loss = 29663.404297, Accuracy = 0.802000045776\n",
      "Performance on test set: Training epochs #1680000, Batch Loss = 14425.46875, Accuracy = 0.943000078201\n",
      "Training epochs #1700000: Batch Loss = 238220.906250, Accuracy = 0.242999985814\n",
      "Performance on test set: Training epochs #1700000, Batch Loss = 9587.765625, Accuracy = 0.943000078201\n",
      "Training epochs #1720000: Batch Loss = 41957.929688, Accuracy = 0.738999962807\n",
      "Performance on test set: Training epochs #1720000, Batch Loss = 9389.28027344, Accuracy = 0.943000018597\n",
      "Training epochs #1740000: Batch Loss = 36628.792969, Accuracy = 0.776000022888\n",
      "Performance on test set: Training epochs #1740000, Batch Loss = 12168.8740234, Accuracy = 0.943000078201\n",
      "Training epochs #1760000: Batch Loss = 30899.609375, Accuracy = 0.824000000954\n",
      "Performance on test set: Training epochs #1760000, Batch Loss = 13344.1210938, Accuracy = 0.943000078201\n",
      "Training epochs #1780000: Batch Loss = 217224.687500, Accuracy = 0.266000002623\n",
      "Performance on test set: Training epochs #1780000, Batch Loss = 8594.265625, Accuracy = 0.943000078201\n",
      "Training epochs #1800000: Batch Loss = 45119.105469, Accuracy = 0.704999983311\n",
      "Performance on test set: Training epochs #1800000, Batch Loss = 8532.73144531, Accuracy = 0.943000078201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #1820000: Batch Loss = 36803.355469, Accuracy = 0.758000016212\n",
      "Performance on test set: Training epochs #1820000, Batch Loss = 11329.4394531, Accuracy = 0.943000018597\n",
      "Training epochs #1840000: Batch Loss = 25269.902344, Accuracy = 0.811999976635\n",
      "Performance on test set: Training epochs #1840000, Batch Loss = 12342.8095703, Accuracy = 0.943000078201\n",
      "Training epochs #1860000: Batch Loss = 198271.953125, Accuracy = 0.264999985695\n",
      "Performance on test set: Training epochs #1860000, Batch Loss = 7642.79296875, Accuracy = 0.954999983311\n",
      "Training epochs #1880000: Batch Loss = 40341.414062, Accuracy = 0.726999998093\n",
      "Performance on test set: Training epochs #1880000, Batch Loss = 7724.90625, Accuracy = 0.943000018597\n",
      "Training epochs #1900000: Batch Loss = 31597.232422, Accuracy = 0.789000034332\n",
      "Performance on test set: Training epochs #1900000, Batch Loss = 10542.8232422, Accuracy = 0.942999958992\n",
      "Training epochs #1920000: Batch Loss = 21955.945312, Accuracy = 0.848000049591\n",
      "Performance on test set: Training epochs #1920000, Batch Loss = 11362.0800781, Accuracy = 0.943000018597\n",
      "Training epochs #1940000: Batch Loss = 174299.015625, Accuracy = 0.293999999762\n",
      "Performance on test set: Training epochs #1940000, Batch Loss = 7155.81298828, Accuracy = 0.955000042915\n",
      "Training epochs #1960000: Batch Loss = 31059.402344, Accuracy = 0.757000029087\n",
      "Performance on test set: Training epochs #1960000, Batch Loss = 7210.60839844, Accuracy = 0.954999983311\n",
      "Training epochs #1980000: Batch Loss = 25618.269531, Accuracy = 0.804000020027\n",
      "Performance on test set: Training epochs #1980000, Batch Loss = 9635.36132812, Accuracy = 0.942999958992\n",
      "Training epochs #2000000: Batch Loss = 21328.693359, Accuracy = 0.846000015736\n",
      "Performance on test set: Training epochs #2000000, Batch Loss = 10532.6376953, Accuracy = 0.942999958992\n",
      "Training epochs #2020000: Batch Loss = 178268.312500, Accuracy = 0.287999987602\n",
      "Performance on test set: Training epochs #2020000, Batch Loss = 6820.47167969, Accuracy = 0.954999983311\n",
      "Training epochs #2040000: Batch Loss = 34094.160156, Accuracy = 0.730999946594\n",
      "Performance on test set: Training epochs #2040000, Batch Loss = 6834.74316406, Accuracy = 0.954999983311\n",
      "Training epochs #2060000: Batch Loss = 25749.414062, Accuracy = 0.791000008583\n",
      "Performance on test set: Training epochs #2060000, Batch Loss = 8852.00292969, Accuracy = 0.943000078201\n",
      "Training epochs #2080000: Batch Loss = 22164.339844, Accuracy = 0.843000054359\n",
      "Performance on test set: Training epochs #2080000, Batch Loss = 9612.16308594, Accuracy = 0.943000078201\n",
      "Training epochs #2100000: Batch Loss = 163596.515625, Accuracy = 0.303000003099\n",
      "Performance on test set: Training epochs #2100000, Batch Loss = 6165.15576172, Accuracy = 0.955000042915\n",
      "Training epochs #2120000: Batch Loss = 28601.457031, Accuracy = 0.751999974251\n",
      "Performance on test set: Training epochs #2120000, Batch Loss = 6386.60351562, Accuracy = 0.955000042915\n",
      "Training epochs #2140000: Batch Loss = 22680.898438, Accuracy = 0.822000026703\n",
      "Performance on test set: Training epochs #2140000, Batch Loss = 8146.66845703, Accuracy = 0.954999983311\n",
      "Training epochs #2160000: Batch Loss = 20179.226562, Accuracy = 0.846000015736\n",
      "Performance on test set: Training epochs #2160000, Batch Loss = 8796.76757812, Accuracy = 0.954999983311\n",
      "Training epochs #2180000: Batch Loss = 152782.906250, Accuracy = 0.29699999094\n",
      "Performance on test set: Training epochs #2180000, Batch Loss = 5538.52294922, Accuracy = 0.955000042915\n",
      "Training epochs #2200000: Batch Loss = 29996.525391, Accuracy = 0.746999979019\n",
      "Performance on test set: Training epochs #2200000, Batch Loss = 6016.99316406, Accuracy = 0.955000042915\n",
      "Training epochs #2220000: Batch Loss = 18744.773438, Accuracy = 0.831000030041\n",
      "Performance on test set: Training epochs #2220000, Batch Loss = 7891.68261719, Accuracy = 0.955000042915\n",
      "Training epochs #2240000: Batch Loss = 17437.824219, Accuracy = 0.855000019073\n",
      "Performance on test set: Training epochs #2240000, Batch Loss = 8391.63183594, Accuracy = 0.955000042915\n",
      "Training epochs #2260000: Batch Loss = 144331.703125, Accuracy = 0.324999988079\n",
      "Performance on test set: Training epochs #2260000, Batch Loss = 4996.55712891, Accuracy = 0.954999983311\n",
      "Training epochs #2280000: Batch Loss = 28397.242188, Accuracy = 0.741999983788\n",
      "Performance on test set: Training epochs #2280000, Batch Loss = 5572.52539062, Accuracy = 0.954999983311\n",
      "Training epochs #2300000: Batch Loss = 20225.121094, Accuracy = 0.823000013828\n",
      "Performance on test set: Training epochs #2300000, Batch Loss = 7579.67333984, Accuracy = 0.955000042915\n",
      "Training epochs #2320000: Batch Loss = 13614.998047, Accuracy = 0.869000017643\n",
      "Performance on test set: Training epochs #2320000, Batch Loss = 8166.67871094, Accuracy = 0.954999983311\n",
      "Training epochs #2340000: Batch Loss = 133782.359375, Accuracy = 0.338999986649\n",
      "Performance on test set: Training epochs #2340000, Batch Loss = 4772.02734375, Accuracy = 0.954999983311\n",
      "Training epochs #2360000: Batch Loss = 24272.222656, Accuracy = 0.760999977589\n",
      "Performance on test set: Training epochs #2360000, Batch Loss = 5176.50195312, Accuracy = 0.954999983311\n",
      "Training epochs #2380000: Batch Loss = 20385.744141, Accuracy = 0.84600007534\n",
      "Performance on test set: Training epochs #2380000, Batch Loss = 7210.72363281, Accuracy = 0.954999983311\n",
      "Training epochs #2400000: Batch Loss = 12941.254883, Accuracy = 0.881999969482\n",
      "Performance on test set: Training epochs #2400000, Batch Loss = 7718.34082031, Accuracy = 0.954999983311\n",
      "Training epochs #2420000: Batch Loss = 129212.859375, Accuracy = 0.34999999404\n",
      "Performance on test set: Training epochs #2420000, Batch Loss = 4198.59179688, Accuracy = 0.954999983311\n",
      "Training epochs #2440000: Batch Loss = 24405.923828, Accuracy = 0.763999998569\n",
      "Performance on test set: Training epochs #2440000, Batch Loss = 4727.86083984, Accuracy = 0.954999983311\n",
      "Training epochs #2460000: Batch Loss = 18056.568359, Accuracy = 0.836000025272\n",
      "Performance on test set: Training epochs #2460000, Batch Loss = 6922.55371094, Accuracy = 0.954999983311\n",
      "Training epochs #2480000: Batch Loss = 12543.831055, Accuracy = 0.87600004673\n",
      "Performance on test set: Training epochs #2480000, Batch Loss = 7434.47119141, Accuracy = 0.954999983311\n",
      "Training epochs #2500000: Batch Loss = 115708.421875, Accuracy = 0.338999986649\n",
      "Performance on test set: Training epochs #2500000, Batch Loss = 3730.65673828, Accuracy = 0.954999983311\n",
      "Training epochs #2520000: Batch Loss = 21811.828125, Accuracy = 0.769999980927\n",
      "Performance on test set: Training epochs #2520000, Batch Loss = 4487.49121094, Accuracy = 0.955000042915\n",
      "Training epochs #2540000: Batch Loss = 16182.903320, Accuracy = 0.874000012875\n",
      "Performance on test set: Training epochs #2540000, Batch Loss = 6614.68994141, Accuracy = 0.954999983311\n",
      "Training epochs #2560000: Batch Loss = 12683.121094, Accuracy = 0.87600004673\n",
      "Performance on test set: Training epochs #2560000, Batch Loss = 7066.10058594, Accuracy = 0.95500010252\n",
      "Training epochs #2580000: Batch Loss = 107290.484375, Accuracy = 0.39200001955\n",
      "Performance on test set: Training epochs #2580000, Batch Loss = 3403.44628906, Accuracy = 0.955000042915\n",
      "Training epochs #2600000: Batch Loss = 20135.763672, Accuracy = 0.763999938965\n",
      "Performance on test set: Training epochs #2600000, Batch Loss = 4045.21728516, Accuracy = 0.954999983311\n",
      "Training epochs #2620000: Batch Loss = 16798.826172, Accuracy = 0.84500002861\n",
      "Performance on test set: Training epochs #2620000, Batch Loss = 6182.77441406, Accuracy = 0.954999983311\n",
      "Training epochs #2640000: Batch Loss = 11106.653320, Accuracy = 0.876999974251\n",
      "Performance on test set: Training epochs #2640000, Batch Loss = 6652.19824219, Accuracy = 0.95500010252\n",
      "Training epochs #2660000: Batch Loss = 100146.718750, Accuracy = 0.39099997282\n",
      "Performance on test set: Training epochs #2660000, Batch Loss = 2994.96777344, Accuracy = 0.955000042915\n",
      "Training epochs #2680000: Batch Loss = 17424.917969, Accuracy = 0.793000042439\n",
      "Performance on test set: Training epochs #2680000, Batch Loss = 3674.75268555, Accuracy = 0.955000042915\n",
      "Training epochs #2700000: Batch Loss = 15771.490234, Accuracy = 0.851000070572\n",
      "Performance on test set: Training epochs #2700000, Batch Loss = 5751.90820312, Accuracy = 0.954999983311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #2720000: Batch Loss = 12501.528320, Accuracy = 0.870000004768\n",
      "Performance on test set: Training epochs #2720000, Batch Loss = 6199.30273438, Accuracy = 0.95500010252\n",
      "Training epochs #2740000: Batch Loss = 95800.375000, Accuracy = 0.388000011444\n",
      "Performance on test set: Training epochs #2740000, Batch Loss = 2790.38964844, Accuracy = 0.955000042915\n",
      "Training epochs #2760000: Batch Loss = 16520.492188, Accuracy = 0.806999981403\n",
      "Performance on test set: Training epochs #2760000, Batch Loss = 3481.21313477, Accuracy = 0.954999983311\n",
      "Training epochs #2780000: Batch Loss = 13250.332031, Accuracy = 0.875\n",
      "Performance on test set: Training epochs #2780000, Batch Loss = 5400.77832031, Accuracy = 0.955000042915\n",
      "Training epochs #2800000: Batch Loss = 9911.195312, Accuracy = 0.888000011444\n",
      "Performance on test set: Training epochs #2800000, Batch Loss = 5723.46435547, Accuracy = 0.954999983311\n",
      "Training epochs #2820000: Batch Loss = 89814.484375, Accuracy = 0.376000016928\n",
      "Performance on test set: Training epochs #2820000, Batch Loss = 2452.99707031, Accuracy = 0.95500010252\n",
      "Training epochs #2840000: Batch Loss = 15764.193359, Accuracy = 0.797000050545\n",
      "Performance on test set: Training epochs #2840000, Batch Loss = 3236.43310547, Accuracy = 0.954999983311\n",
      "Training epochs #2860000: Batch Loss = 13756.435547, Accuracy = 0.85900002718\n",
      "Performance on test set: Training epochs #2860000, Batch Loss = 4979.50488281, Accuracy = 0.955000042915\n",
      "Training epochs #2880000: Batch Loss = 7470.829590, Accuracy = 0.905999958515\n",
      "Performance on test set: Training epochs #2880000, Batch Loss = 5367.77539062, Accuracy = 0.955000042915\n",
      "Training epochs #2900000: Batch Loss = 82747.531250, Accuracy = 0.41400000453\n",
      "Performance on test set: Training epochs #2900000, Batch Loss = 2387.05175781, Accuracy = 0.955000042915\n",
      "Training epochs #2920000: Batch Loss = 16090.355469, Accuracy = 0.81200003624\n",
      "Performance on test set: Training epochs #2920000, Batch Loss = 2975.79760742, Accuracy = 0.954999983311\n",
      "Training epochs #2940000: Batch Loss = 11731.416992, Accuracy = 0.882000029087\n",
      "Performance on test set: Training epochs #2940000, Batch Loss = 4632.80957031, Accuracy = 0.954999983311\n",
      "Training epochs #2960000: Batch Loss = 8067.365723, Accuracy = 0.894999980927\n",
      "Performance on test set: Training epochs #2960000, Batch Loss = 4948.40332031, Accuracy = 0.955000042915\n",
      "Training epochs #2980000: Batch Loss = 85255.546875, Accuracy = 0.409000009298\n",
      "Performance on test set: Training epochs #2980000, Batch Loss = 2102.09570312, Accuracy = 0.961000025272\n",
      "Training epochs #3000000: Batch Loss = 13737.090820, Accuracy = 0.814999997616\n",
      "Performance on test set: Training epochs #3000000, Batch Loss = 2726.74047852, Accuracy = 0.954999983311\n",
      "Training epochs #3020000: Batch Loss = 10278.451172, Accuracy = 0.87399995327\n",
      "Performance on test set: Training epochs #3020000, Batch Loss = 4254.77783203, Accuracy = 0.954999983311\n",
      "Training epochs #3040000: Batch Loss = 7568.135742, Accuracy = 0.914000034332\n",
      "Performance on test set: Training epochs #3040000, Batch Loss = 4522.60644531, Accuracy = 0.955000042915\n",
      "Training epochs #3060000: Batch Loss = 72166.054688, Accuracy = 0.446000009775\n",
      "Performance on test set: Training epochs #3060000, Batch Loss = 1867.94055176, Accuracy = 0.961000084877\n",
      "Training epochs #3080000: Batch Loss = 11132.917969, Accuracy = 0.844000041485\n",
      "Performance on test set: Training epochs #3080000, Batch Loss = 2555.23925781, Accuracy = 0.954999983311\n",
      "Training epochs #3100000: Batch Loss = 9287.019531, Accuracy = 0.891000032425\n",
      "Performance on test set: Training epochs #3100000, Batch Loss = 3963.75585938, Accuracy = 0.955000042915\n",
      "Training epochs #3120000: Batch Loss = 6883.668457, Accuracy = 0.916000068188\n",
      "Performance on test set: Training epochs #3120000, Batch Loss = 4158.95166016, Accuracy = 0.955000042915\n",
      "Training epochs #3140000: Batch Loss = 67914.812500, Accuracy = 0.458000004292\n",
      "Performance on test set: Training epochs #3140000, Batch Loss = 1782.9083252, Accuracy = 0.961000025272\n",
      "Training epochs #3160000: Batch Loss = 11505.493164, Accuracy = 0.830000042915\n",
      "Performance on test set: Training epochs #3160000, Batch Loss = 2380.07714844, Accuracy = 0.955000042915\n",
      "Training epochs #3180000: Batch Loss = 10342.646484, Accuracy = 0.869000017643\n",
      "Performance on test set: Training epochs #3180000, Batch Loss = 3702.26245117, Accuracy = 0.955000042915\n",
      "Training epochs #3200000: Batch Loss = 7525.315430, Accuracy = 0.900000035763\n",
      "Performance on test set: Training epochs #3200000, Batch Loss = 3874.3425293, Accuracy = 0.954999983311\n",
      "Training epochs #3220000: Batch Loss = 64610.906250, Accuracy = 0.455000013113\n",
      "Performance on test set: Training epochs #3220000, Batch Loss = 1587.9420166, Accuracy = 0.961000025272\n",
      "Training epochs #3240000: Batch Loss = 11778.106445, Accuracy = 0.842000067234\n",
      "Performance on test set: Training epochs #3240000, Batch Loss = 2244.03515625, Accuracy = 0.955000042915\n",
      "Training epochs #3260000: Batch Loss = 8484.557617, Accuracy = 0.891000032425\n",
      "Performance on test set: Training epochs #3260000, Batch Loss = 3482.79760742, Accuracy = 0.955000042915\n",
      "Training epochs #3280000: Batch Loss = 7076.821289, Accuracy = 0.909000039101\n",
      "Performance on test set: Training epochs #3280000, Batch Loss = 3598.73388672, Accuracy = 0.954999923706\n",
      "Training epochs #3300000: Batch Loss = 66714.640625, Accuracy = 0.43900001049\n",
      "Performance on test set: Training epochs #3300000, Batch Loss = 1476.63330078, Accuracy = 0.961000025272\n",
      "Training epochs #3320000: Batch Loss = 11934.768555, Accuracy = 0.826000094414\n",
      "Performance on test set: Training epochs #3320000, Batch Loss = 2114.18457031, Accuracy = 0.961000025272\n",
      "Training epochs #3340000: Batch Loss = 8778.122070, Accuracy = 0.894000053406\n",
      "Performance on test set: Training epochs #3340000, Batch Loss = 3314.69482422, Accuracy = 0.955000042915\n",
      "Training epochs #3360000: Batch Loss = 4961.456543, Accuracy = 0.935000121593\n",
      "Performance on test set: Training epochs #3360000, Batch Loss = 3358.19287109, Accuracy = 0.955000042915\n",
      "Training epochs #3380000: Batch Loss = 58854.671875, Accuracy = 0.449999988079\n",
      "Performance on test set: Training epochs #3380000, Batch Loss = 1398.39782715, Accuracy = 0.975000023842\n",
      "Training epochs #3400000: Batch Loss = 12450.799805, Accuracy = 0.834000051022\n",
      "Performance on test set: Training epochs #3400000, Batch Loss = 1954.51806641, Accuracy = 0.961000025272\n",
      "Training epochs #3420000: Batch Loss = 9265.353516, Accuracy = 0.893999993801\n",
      "Performance on test set: Training epochs #3420000, Batch Loss = 3118.52416992, Accuracy = 0.954999983311\n",
      "Training epochs #3440000: Batch Loss = 5191.403809, Accuracy = 0.921000003815\n",
      "Performance on test set: Training epochs #3440000, Batch Loss = 3166.14160156, Accuracy = 0.955000042915\n",
      "Training epochs #3460000: Batch Loss = 56683.453125, Accuracy = 0.493000000715\n",
      "Performance on test set: Training epochs #3460000, Batch Loss = 1295.45544434, Accuracy = 0.975000083447\n",
      "Training epochs #3480000: Batch Loss = 8816.853516, Accuracy = 0.852999985218\n",
      "Performance on test set: Training epochs #3480000, Batch Loss = 1937.63623047, Accuracy = 0.961000025272\n",
      "Training epochs #3500000: Batch Loss = 7129.054688, Accuracy = 0.892000079155\n",
      "Performance on test set: Training epochs #3500000, Batch Loss = 2933.26000977, Accuracy = 0.955000042915\n",
      "Training epochs #3520000: Batch Loss = 5568.967773, Accuracy = 0.919000029564\n",
      "Performance on test set: Training epochs #3520000, Batch Loss = 2966.74584961, Accuracy = 0.955000042915\n",
      "Training epochs #3540000: Batch Loss = 53323.742188, Accuracy = 0.504999995232\n",
      "Performance on test set: Training epochs #3540000, Batch Loss = 1246.81799316, Accuracy = 0.975000023842\n",
      "Training epochs #3560000: Batch Loss = 7564.796387, Accuracy = 0.882000029087\n",
      "Performance on test set: Training epochs #3560000, Batch Loss = 1837.27905273, Accuracy = 0.961000025272\n",
      "Training epochs #3580000: Batch Loss = 9661.929688, Accuracy = 0.897000074387\n",
      "Performance on test set: Training epochs #3580000, Batch Loss = 2820.86962891, Accuracy = 0.955000042915\n",
      "Training epochs #3600000: Batch Loss = 4291.885254, Accuracy = 0.94000005722\n",
      "Performance on test set: Training epochs #3600000, Batch Loss = 2871.93188477, Accuracy = 0.954999983311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #3620000: Batch Loss = 52645.800781, Accuracy = 0.512000083923\n",
      "Performance on test set: Training epochs #3620000, Batch Loss = 1194.82067871, Accuracy = 0.975000023842\n",
      "Training epochs #3640000: Batch Loss = 9820.023438, Accuracy = 0.849999964237\n",
      "Performance on test set: Training epochs #3640000, Batch Loss = 1693.46972656, Accuracy = 0.961000025272\n",
      "Training epochs #3660000: Batch Loss = 8141.970703, Accuracy = 0.899000048637\n",
      "Performance on test set: Training epochs #3660000, Batch Loss = 2641.17993164, Accuracy = 0.961000025272\n",
      "Training epochs #3680000: Batch Loss = 6052.024902, Accuracy = 0.909000098705\n",
      "Performance on test set: Training epochs #3680000, Batch Loss = 2666.49389648, Accuracy = 0.961000025272\n",
      "Training epochs #3700000: Batch Loss = 52238.109375, Accuracy = 0.509000003338\n",
      "Performance on test set: Training epochs #3700000, Batch Loss = 1157.17553711, Accuracy = 0.975000023842\n",
      "Training epochs #3720000: Batch Loss = 7656.916992, Accuracy = 0.860999941826\n",
      "Performance on test set: Training epochs #3720000, Batch Loss = 1568.81274414, Accuracy = 0.961000025272\n",
      "Training epochs #3740000: Batch Loss = 5622.037109, Accuracy = 0.909000039101\n",
      "Performance on test set: Training epochs #3740000, Batch Loss = 2528.65649414, Accuracy = 0.961000025272\n",
      "Training epochs #3760000: Batch Loss = 4565.032715, Accuracy = 0.923999965191\n",
      "Performance on test set: Training epochs #3760000, Batch Loss = 2557.54345703, Accuracy = 0.961000025272\n",
      "Training epochs #3780000: Batch Loss = 46789.902344, Accuracy = 0.520000040531\n",
      "Performance on test set: Training epochs #3780000, Batch Loss = 1106.7590332, Accuracy = 0.975000023842\n",
      "Training epochs #3800000: Batch Loss = 7997.284668, Accuracy = 0.857000052929\n",
      "Performance on test set: Training epochs #3800000, Batch Loss = 1370.75488281, Accuracy = 0.961000025272\n",
      "Training epochs #3820000: Batch Loss = 6554.714844, Accuracy = 0.908999979496\n",
      "Performance on test set: Training epochs #3820000, Batch Loss = 2332.60180664, Accuracy = 0.961000025272\n",
      "Training epochs #3840000: Batch Loss = 4268.497070, Accuracy = 0.920000016689\n",
      "Performance on test set: Training epochs #3840000, Batch Loss = 2420.57617188, Accuracy = 0.961000025272\n",
      "Training epochs #3860000: Batch Loss = 43017.011719, Accuracy = 0.535000026226\n",
      "Performance on test set: Training epochs #3860000, Batch Loss = 1078.69604492, Accuracy = 0.975000023842\n",
      "Training epochs #3880000: Batch Loss = 7398.233887, Accuracy = 0.880000054836\n",
      "Performance on test set: Training epochs #3880000, Batch Loss = 1288.84863281, Accuracy = 0.961000025272\n",
      "Training epochs #3900000: Batch Loss = 5354.899414, Accuracy = 0.915000081062\n",
      "Performance on test set: Training epochs #3900000, Batch Loss = 2241.66650391, Accuracy = 0.961000084877\n",
      "Training epochs #3920000: Batch Loss = 3491.589844, Accuracy = 0.928000092506\n",
      "Performance on test set: Training epochs #3920000, Batch Loss = 2250.08666992, Accuracy = 0.961000025272\n",
      "Training epochs #3940000: Batch Loss = 43696.000000, Accuracy = 0.544000029564\n",
      "Performance on test set: Training epochs #3940000, Batch Loss = 1052.11437988, Accuracy = 0.975000083447\n",
      "Training epochs #3960000: Batch Loss = 5765.988281, Accuracy = 0.888000071049\n",
      "Performance on test set: Training epochs #3960000, Batch Loss = 1168.36694336, Accuracy = 0.961000025272\n",
      "Training epochs #3980000: Batch Loss = 6501.294922, Accuracy = 0.910000026226\n",
      "Performance on test set: Training epochs #3980000, Batch Loss = 2094.41821289, Accuracy = 0.961000025272\n",
      "Training epochs #4000000: Batch Loss = 4237.249023, Accuracy = 0.943000018597\n",
      "Performance on test set: Training epochs #4000000, Batch Loss = 2150.18310547, Accuracy = 0.961000025272\n",
      "Training epochs #4020000: Batch Loss = 39831.582031, Accuracy = 0.540000021458\n",
      "Performance on test set: Training epochs #4020000, Batch Loss = 996.650146484, Accuracy = 0.975000023842\n",
      "Training epochs #4040000: Batch Loss = 5756.146973, Accuracy = 0.886000096798\n",
      "Performance on test set: Training epochs #4040000, Batch Loss = 1069.41491699, Accuracy = 0.961000025272\n",
      "Training epochs #4060000: Batch Loss = 5027.819824, Accuracy = 0.913000047207\n",
      "Performance on test set: Training epochs #4060000, Batch Loss = 1942.49902344, Accuracy = 0.961000025272\n",
      "Training epochs #4080000: Batch Loss = 2805.653320, Accuracy = 0.942000031471\n",
      "Performance on test set: Training epochs #4080000, Batch Loss = 2007.18994141, Accuracy = 0.961000025272\n",
      "Training epochs #4100000: Batch Loss = 37883.128906, Accuracy = 0.545000016689\n",
      "Performance on test set: Training epochs #4100000, Batch Loss = 956.970581055, Accuracy = 0.975000023842\n",
      "Training epochs #4120000: Batch Loss = 6792.685059, Accuracy = 0.878000080585\n",
      "Performance on test set: Training epochs #4120000, Batch Loss = 1075.22167969, Accuracy = 0.961000084877\n",
      "Training epochs #4140000: Batch Loss = 5385.746094, Accuracy = 0.902999997139\n",
      "Performance on test set: Training epochs #4140000, Batch Loss = 1896.19055176, Accuracy = 0.961000025272\n",
      "Training epochs #4160000: Batch Loss = 3379.588379, Accuracy = 0.933000087738\n",
      "Performance on test set: Training epochs #4160000, Batch Loss = 1918.97924805, Accuracy = 0.961000025272\n",
      "Training epochs #4180000: Batch Loss = 37811.062500, Accuracy = 0.56099998951\n",
      "Performance on test set: Training epochs #4180000, Batch Loss = 942.421142578, Accuracy = 0.975000023842\n",
      "Training epochs #4200000: Batch Loss = 4923.601562, Accuracy = 0.897000074387\n",
      "Performance on test set: Training epochs #4200000, Batch Loss = 1008.92687988, Accuracy = 0.961000025272\n",
      "Training epochs #4220000: Batch Loss = 3903.417480, Accuracy = 0.930000066757\n",
      "Performance on test set: Training epochs #4220000, Batch Loss = 1816.09912109, Accuracy = 0.961000025272\n",
      "Training epochs #4240000: Batch Loss = 2869.347168, Accuracy = 0.942000031471\n",
      "Performance on test set: Training epochs #4240000, Batch Loss = 1797.40661621, Accuracy = 0.961000025272\n",
      "Training epochs #4260000: Batch Loss = 36092.207031, Accuracy = 0.566000044346\n",
      "Performance on test set: Training epochs #4260000, Batch Loss = 897.637756348, Accuracy = 0.975000023842\n",
      "Training epochs #4280000: Batch Loss = 5985.231934, Accuracy = 0.8900000453\n",
      "Performance on test set: Training epochs #4280000, Batch Loss = 967.010864258, Accuracy = 0.975000023842\n",
      "Training epochs #4300000: Batch Loss = 5250.998535, Accuracy = 0.917999982834\n",
      "Performance on test set: Training epochs #4300000, Batch Loss = 1700.66711426, Accuracy = 0.961000025272\n",
      "Training epochs #4320000: Batch Loss = 2855.412109, Accuracy = 0.955000042915\n",
      "Performance on test set: Training epochs #4320000, Batch Loss = 1741.11914062, Accuracy = 0.961000025272\n",
      "Training epochs #4340000: Batch Loss = 37986.386719, Accuracy = 0.552999973297\n",
      "Performance on test set: Training epochs #4340000, Batch Loss = 878.893127441, Accuracy = 0.974999964237\n",
      "Training epochs #4360000: Batch Loss = 5795.219727, Accuracy = 0.880999982357\n",
      "Performance on test set: Training epochs #4360000, Batch Loss = 959.393493652, Accuracy = 0.975000023842\n",
      "Training epochs #4380000: Batch Loss = 4232.116211, Accuracy = 0.925999999046\n",
      "Performance on test set: Training epochs #4380000, Batch Loss = 1622.05029297, Accuracy = 0.961000025272\n",
      "Training epochs #4400000: Batch Loss = 2852.122070, Accuracy = 0.945000052452\n",
      "Performance on test set: Training epochs #4400000, Batch Loss = 1612.87255859, Accuracy = 0.961000025272\n",
      "Training epochs #4420000: Batch Loss = 31600.292969, Accuracy = 0.597000002861\n",
      "Performance on test set: Training epochs #4420000, Batch Loss = 788.558105469, Accuracy = 0.975000023842\n",
      "Training epochs #4440000: Batch Loss = 5478.958984, Accuracy = 0.899000048637\n",
      "Performance on test set: Training epochs #4440000, Batch Loss = 890.553710938, Accuracy = 0.975000023842\n",
      "Training epochs #4460000: Batch Loss = 4498.831055, Accuracy = 0.925999999046\n",
      "Performance on test set: Training epochs #4460000, Batch Loss = 1499.36193848, Accuracy = 0.961000084877\n",
      "Training epochs #4480000: Batch Loss = 2152.129395, Accuracy = 0.948000013828\n",
      "Performance on test set: Training epochs #4480000, Batch Loss = 1506.9942627, Accuracy = 0.961000025272\n",
      "Training epochs #4500000: Batch Loss = 33906.046875, Accuracy = 0.599999964237\n",
      "Performance on test set: Training epochs #4500000, Batch Loss = 749.593505859, Accuracy = 0.975000023842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #4520000: Batch Loss = 3654.944824, Accuracy = 0.912000119686\n",
      "Performance on test set: Training epochs #4520000, Batch Loss = 840.705078125, Accuracy = 0.975000023842\n",
      "Training epochs #4540000: Batch Loss = 5132.520020, Accuracy = 0.925999999046\n",
      "Performance on test set: Training epochs #4540000, Batch Loss = 1431.0123291, Accuracy = 0.961000025272\n",
      "Training epochs #4560000: Batch Loss = 2387.990234, Accuracy = 0.95400005579\n",
      "Performance on test set: Training epochs #4560000, Batch Loss = 1392.09899902, Accuracy = 0.961000025272\n",
      "Training epochs #4580000: Batch Loss = 29674.585938, Accuracy = 0.588999986649\n",
      "Performance on test set: Training epochs #4580000, Batch Loss = 701.115234375, Accuracy = 0.982000112534\n",
      "Training epochs #4600000: Batch Loss = 5274.710938, Accuracy = 0.896000087261\n",
      "Performance on test set: Training epochs #4600000, Batch Loss = 784.384399414, Accuracy = 0.975000023842\n",
      "Training epochs #4620000: Batch Loss = 3216.113770, Accuracy = 0.93700003624\n",
      "Performance on test set: Training epochs #4620000, Batch Loss = 1320.42272949, Accuracy = 0.961000025272\n",
      "Training epochs #4640000: Batch Loss = 3105.843506, Accuracy = 0.933999955654\n",
      "Performance on test set: Training epochs #4640000, Batch Loss = 1337.40930176, Accuracy = 0.961000025272\n",
      "Training epochs #4660000: Batch Loss = 30704.175781, Accuracy = 0.610000014305\n",
      "Performance on test set: Training epochs #4660000, Batch Loss = 667.280578613, Accuracy = 0.981999993324\n",
      "Training epochs #4680000: Batch Loss = 5484.297852, Accuracy = 0.899000048637\n",
      "Performance on test set: Training epochs #4680000, Batch Loss = 769.645019531, Accuracy = 0.980000138283\n",
      "Training epochs #4700000: Batch Loss = 3427.881348, Accuracy = 0.93800008297\n",
      "Performance on test set: Training epochs #4700000, Batch Loss = 1270.37451172, Accuracy = 0.961000025272\n",
      "Training epochs #4720000: Batch Loss = 2020.219727, Accuracy = 0.956000089645\n",
      "Performance on test set: Training epochs #4720000, Batch Loss = 1242.07409668, Accuracy = 0.961000025272\n",
      "Training epochs #4740000: Batch Loss = 28469.419922, Accuracy = 0.619999945164\n",
      "Performance on test set: Training epochs #4740000, Batch Loss = 639.479553223, Accuracy = 0.982000052929\n",
      "Training epochs #4760000: Batch Loss = 3813.974609, Accuracy = 0.920000076294\n",
      "Performance on test set: Training epochs #4760000, Batch Loss = 739.379638672, Accuracy = 0.982000052929\n",
      "Training epochs #4780000: Batch Loss = 4501.815430, Accuracy = 0.93900001049\n",
      "Performance on test set: Training epochs #4780000, Batch Loss = 1202.50085449, Accuracy = 0.961000025272\n",
      "Training epochs #4800000: Batch Loss = 2347.557373, Accuracy = 0.93599998951\n",
      "Performance on test set: Training epochs #4800000, Batch Loss = 1140.50170898, Accuracy = 0.961000025272\n",
      "Training epochs #4820000: Batch Loss = 26356.291016, Accuracy = 0.605000019073\n",
      "Performance on test set: Training epochs #4820000, Batch Loss = 610.733154297, Accuracy = 0.981999993324\n",
      "Training epochs #4840000: Batch Loss = 4420.655762, Accuracy = 0.913000047207\n",
      "Performance on test set: Training epochs #4840000, Batch Loss = 697.428588867, Accuracy = 0.982000112534\n",
      "Training epochs #4860000: Batch Loss = 3982.512695, Accuracy = 0.931000053883\n",
      "Performance on test set: Training epochs #4860000, Batch Loss = 1091.13232422, Accuracy = 0.961000025272\n",
      "Training epochs #4880000: Batch Loss = 2723.125977, Accuracy = 0.945000112057\n",
      "Performance on test set: Training epochs #4880000, Batch Loss = 1004.43383789, Accuracy = 0.975000083447\n",
      "Training epochs #4900000: Batch Loss = 26481.800781, Accuracy = 0.60900002718\n",
      "Performance on test set: Training epochs #4900000, Batch Loss = 577.639343262, Accuracy = 0.982000052929\n",
      "Training epochs #4920000: Batch Loss = 3562.217041, Accuracy = 0.929000139236\n",
      "Performance on test set: Training epochs #4920000, Batch Loss = 674.646179199, Accuracy = 0.982000112534\n",
      "Training epochs #4940000: Batch Loss = 4081.468994, Accuracy = 0.9240000844\n",
      "Performance on test set: Training epochs #4940000, Batch Loss = 1030.4230957, Accuracy = 0.961000025272\n",
      "Training epochs #4960000: Batch Loss = 2335.032959, Accuracy = 0.948000073433\n",
      "Performance on test set: Training epochs #4960000, Batch Loss = 1002.87591553, Accuracy = 0.974999964237\n",
      "Training epochs #4980000: Batch Loss = 26632.400391, Accuracy = 0.625\n",
      "Performance on test set: Training epochs #4980000, Batch Loss = 562.30871582, Accuracy = 0.982000112534\n",
      "Training epochs #5000000: Batch Loss = 3705.973389, Accuracy = 0.903999984264\n",
      "Performance on test set: Training epochs #5000000, Batch Loss = 666.855712891, Accuracy = 0.982000112534\n",
      "Training epochs #5020000: Batch Loss = 3757.966064, Accuracy = 0.93900001049\n",
      "Performance on test set: Training epochs #5020000, Batch Loss = 979.64074707, Accuracy = 0.961000025272\n",
      "Training epochs #5040000: Batch Loss = 1662.327393, Accuracy = 0.966000020504\n",
      "Performance on test set: Training epochs #5040000, Batch Loss = 926.911193848, Accuracy = 0.975000023842\n",
      "Training epochs #5060000: Batch Loss = 22241.277344, Accuracy = 0.642000079155\n",
      "Performance on test set: Training epochs #5060000, Batch Loss = 534.337341309, Accuracy = 0.982000052929\n",
      "Training epochs #5080000: Batch Loss = 3692.986816, Accuracy = 0.918000042439\n",
      "Performance on test set: Training epochs #5080000, Batch Loss = 642.852050781, Accuracy = 0.982000052929\n",
      "Training epochs #5100000: Batch Loss = 2949.023926, Accuracy = 0.943000078201\n",
      "Performance on test set: Training epochs #5100000, Batch Loss = 904.905334473, Accuracy = 0.961000025272\n",
      "Training epochs #5120000: Batch Loss = 1612.117676, Accuracy = 0.962000012398\n",
      "Performance on test set: Training epochs #5120000, Batch Loss = 937.875244141, Accuracy = 0.975000023842\n",
      "Training epochs #5140000: Batch Loss = 24013.437500, Accuracy = 0.629000008106\n",
      "Performance on test set: Training epochs #5140000, Batch Loss = 514.617126465, Accuracy = 0.982000052929\n",
      "Training epochs #5160000: Batch Loss = 3156.143066, Accuracy = 0.934000015259\n",
      "Performance on test set: Training epochs #5160000, Batch Loss = 618.551086426, Accuracy = 0.981999993324\n",
      "Training epochs #5180000: Batch Loss = 2814.152832, Accuracy = 0.93900001049\n",
      "Performance on test set: Training epochs #5180000, Batch Loss = 819.647155762, Accuracy = 0.975000023842\n",
      "Training epochs #5200000: Batch Loss = 1475.015625, Accuracy = 0.964999973774\n",
      "Performance on test set: Training epochs #5200000, Batch Loss = 872.271728516, Accuracy = 0.975000023842\n",
      "Training epochs #5220000: Batch Loss = 23370.544922, Accuracy = 0.650999963284\n",
      "Performance on test set: Training epochs #5220000, Batch Loss = 483.18548584, Accuracy = 0.983000099659\n",
      "Training epochs #5240000: Batch Loss = 3035.349609, Accuracy = 0.925000011921\n",
      "Performance on test set: Training epochs #5240000, Batch Loss = 612.46081543, Accuracy = 0.982000112534\n",
      "Training epochs #5260000: Batch Loss = 3046.260986, Accuracy = 0.949000060558\n",
      "Performance on test set: Training epochs #5260000, Batch Loss = 789.279785156, Accuracy = 0.975000023842\n",
      "Training epochs #5280000: Batch Loss = 2351.337646, Accuracy = 0.957000076771\n",
      "Performance on test set: Training epochs #5280000, Batch Loss = 866.239440918, Accuracy = 0.975000023842\n",
      "Training epochs #5300000: Batch Loss = 22662.970703, Accuracy = 0.662999987602\n",
      "Performance on test set: Training epochs #5300000, Batch Loss = 462.213043213, Accuracy = 0.983000040054\n",
      "Training epochs #5320000: Batch Loss = 3646.281006, Accuracy = 0.9240000844\n",
      "Performance on test set: Training epochs #5320000, Batch Loss = 592.116699219, Accuracy = 0.981999993324\n",
      "Training epochs #5340000: Batch Loss = 3808.376709, Accuracy = 0.930000066757\n",
      "Performance on test set: Training epochs #5340000, Batch Loss = 783.085021973, Accuracy = 0.974999964237\n",
      "Training epochs #5360000: Batch Loss = 1435.831299, Accuracy = 0.967000067234\n",
      "Performance on test set: Training epochs #5360000, Batch Loss = 828.327026367, Accuracy = 0.975000023842\n",
      "Training epochs #5380000: Batch Loss = 21187.208984, Accuracy = 0.662000000477\n",
      "Performance on test set: Training epochs #5380000, Batch Loss = 425.723388672, Accuracy = 0.983000040054\n",
      "Training epochs #5400000: Batch Loss = 2445.266846, Accuracy = 0.926000058651\n",
      "Performance on test set: Training epochs #5400000, Batch Loss = 572.523254395, Accuracy = 0.982000052929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #5420000: Batch Loss = 3256.254395, Accuracy = 0.944000005722\n",
      "Performance on test set: Training epochs #5420000, Batch Loss = 722.945800781, Accuracy = 0.975000023842\n",
      "Training epochs #5440000: Batch Loss = 1371.602051, Accuracy = 0.964000046253\n",
      "Performance on test set: Training epochs #5440000, Batch Loss = 766.335083008, Accuracy = 0.975000023842\n",
      "Training epochs #5460000: Batch Loss = 18597.931641, Accuracy = 0.698000013828\n",
      "Performance on test set: Training epochs #5460000, Batch Loss = 402.731658936, Accuracy = 0.990000128746\n",
      "Training epochs #5480000: Batch Loss = 3225.950195, Accuracy = 0.921000003815\n",
      "Performance on test set: Training epochs #5480000, Batch Loss = 579.167541504, Accuracy = 0.981999993324\n",
      "Training epochs #5500000: Batch Loss = 2327.407471, Accuracy = 0.948000073433\n",
      "Performance on test set: Training epochs #5500000, Batch Loss = 753.638061523, Accuracy = 0.975000023842\n",
      "Training epochs #5520000: Batch Loss = 1162.291992, Accuracy = 0.962999999523\n",
      "Performance on test set: Training epochs #5520000, Batch Loss = 779.429077148, Accuracy = 0.975000023842\n",
      "Training epochs #5540000: Batch Loss = 18233.667969, Accuracy = 0.680000007153\n",
      "Performance on test set: Training epochs #5540000, Batch Loss = 386.034667969, Accuracy = 0.990000069141\n",
      "Training epochs #5560000: Batch Loss = 2672.473145, Accuracy = 0.935000061989\n",
      "Performance on test set: Training epochs #5560000, Batch Loss = 543.317321777, Accuracy = 0.982000112534\n",
      "Training epochs #5580000: Batch Loss = 3276.340088, Accuracy = 0.93800008297\n",
      "Performance on test set: Training epochs #5580000, Batch Loss = 703.795166016, Accuracy = 0.974999964237\n",
      "Training epochs #5600000: Batch Loss = 1850.177979, Accuracy = 0.961000025272\n",
      "Performance on test set: Training epochs #5600000, Batch Loss = 725.767822266, Accuracy = 0.975000023842\n",
      "Training epochs #5620000: Batch Loss = 18823.505859, Accuracy = 0.680000007153\n",
      "Performance on test set: Training epochs #5620000, Batch Loss = 383.754669189, Accuracy = 0.990000128746\n",
      "Training epochs #5640000: Batch Loss = 1960.502319, Accuracy = 0.941000044346\n",
      "Performance on test set: Training epochs #5640000, Batch Loss = 526.651367188, Accuracy = 0.982000052929\n",
      "Training epochs #5660000: Batch Loss = 2825.316406, Accuracy = 0.944000005722\n",
      "Performance on test set: Training epochs #5660000, Batch Loss = 673.408874512, Accuracy = 0.975000023842\n",
      "Training epochs #5680000: Batch Loss = 1630.268799, Accuracy = 0.964000046253\n",
      "Performance on test set: Training epochs #5680000, Batch Loss = 705.409606934, Accuracy = 0.975000083447\n",
      "Training epochs #5700000: Batch Loss = 17858.250000, Accuracy = 0.699000000954\n",
      "Performance on test set: Training epochs #5700000, Batch Loss = 357.069580078, Accuracy = 0.990000069141\n",
      "Training epochs #5720000: Batch Loss = 2426.457275, Accuracy = 0.935000002384\n",
      "Performance on test set: Training epochs #5720000, Batch Loss = 499.468048096, Accuracy = 0.982000052929\n",
      "Training epochs #5740000: Batch Loss = 3538.916748, Accuracy = 0.93700003624\n",
      "Performance on test set: Training epochs #5740000, Batch Loss = 684.200561523, Accuracy = 0.975000023842\n",
      "Training epochs #5760000: Batch Loss = 1021.214844, Accuracy = 0.976000010967\n",
      "Performance on test set: Training epochs #5760000, Batch Loss = 684.309448242, Accuracy = 0.975000023842\n",
      "Training epochs #5780000: Batch Loss = 17282.886719, Accuracy = 0.671000003815\n",
      "Performance on test set: Training epochs #5780000, Batch Loss = 356.53237915, Accuracy = 0.990000009537\n",
      "Training epochs #5800000: Batch Loss = 2216.880859, Accuracy = 0.93599998951\n",
      "Performance on test set: Training epochs #5800000, Batch Loss = 477.113098145, Accuracy = 0.982000112534\n",
      "Training epochs #5820000: Batch Loss = 2531.128418, Accuracy = 0.952000021935\n",
      "Performance on test set: Training epochs #5820000, Batch Loss = 633.043579102, Accuracy = 0.975000023842\n",
      "Training epochs #5840000: Batch Loss = 1080.499268, Accuracy = 0.965000092983\n",
      "Performance on test set: Training epochs #5840000, Batch Loss = 633.722473145, Accuracy = 0.975000023842\n",
      "Training epochs #5860000: Batch Loss = 16783.535156, Accuracy = 0.68900001049\n",
      "Performance on test set: Training epochs #5860000, Batch Loss = 345.067138672, Accuracy = 0.990000069141\n",
      "Training epochs #5880000: Batch Loss = 1920.390503, Accuracy = 0.945000052452\n",
      "Performance on test set: Training epochs #5880000, Batch Loss = 435.00994873, Accuracy = 0.982000052929\n",
      "Training epochs #5900000: Batch Loss = 1892.450073, Accuracy = 0.948000073433\n",
      "Performance on test set: Training epochs #5900000, Batch Loss = 611.085144043, Accuracy = 0.975000023842\n",
      "Training epochs #5920000: Batch Loss = 835.787476, Accuracy = 0.976000010967\n",
      "Performance on test set: Training epochs #5920000, Batch Loss = 628.761169434, Accuracy = 0.975000023842\n",
      "Training epochs #5940000: Batch Loss = 15668.158203, Accuracy = 0.695999979973\n",
      "Performance on test set: Training epochs #5940000, Batch Loss = 323.978302002, Accuracy = 0.990000009537\n",
      "Training epochs #5960000: Batch Loss = 2763.906250, Accuracy = 0.935000061989\n",
      "Performance on test set: Training epochs #5960000, Batch Loss = 448.11605835, Accuracy = 0.982000052929\n",
      "Training epochs #5980000: Batch Loss = 1868.672363, Accuracy = 0.952000021935\n",
      "Performance on test set: Training epochs #5980000, Batch Loss = 602.980529785, Accuracy = 0.975000083447\n",
      "Training epochs #6000000: Batch Loss = 1257.305908, Accuracy = 0.967000007629\n",
      "Performance on test set: Training epochs #6000000, Batch Loss = 604.644287109, Accuracy = 0.975000023842\n",
      "Training epochs #6020000: Batch Loss = 16538.216797, Accuracy = 0.701000034809\n",
      "Performance on test set: Training epochs #6020000, Batch Loss = 333.842529297, Accuracy = 0.991000056267\n",
      "Training epochs #6040000: Batch Loss = 1819.473755, Accuracy = 0.957000017166\n",
      "Performance on test set: Training epochs #6040000, Batch Loss = 424.252288818, Accuracy = 0.982000052929\n",
      "Training epochs #6060000: Batch Loss = 1977.940186, Accuracy = 0.95400005579\n",
      "Performance on test set: Training epochs #6060000, Batch Loss = 554.675109863, Accuracy = 0.975000083447\n",
      "Training epochs #6080000: Batch Loss = 854.168945, Accuracy = 0.973000049591\n",
      "Performance on test set: Training epochs #6080000, Batch Loss = 551.814575195, Accuracy = 0.975000023842\n",
      "Training epochs #6100000: Batch Loss = 15498.621094, Accuracy = 0.712000072002\n",
      "Performance on test set: Training epochs #6100000, Batch Loss = 316.619415283, Accuracy = 0.991000056267\n",
      "Training epochs #6120000: Batch Loss = 2491.840332, Accuracy = 0.942000031471\n",
      "Performance on test set: Training epochs #6120000, Batch Loss = 404.357757568, Accuracy = 0.982000112534\n",
      "Training epochs #6140000: Batch Loss = 2211.575195, Accuracy = 0.953000068665\n",
      "Performance on test set: Training epochs #6140000, Batch Loss = 559.139953613, Accuracy = 0.975000083447\n",
      "Training epochs #6160000: Batch Loss = 905.460083, Accuracy = 0.973000049591\n",
      "Performance on test set: Training epochs #6160000, Batch Loss = 562.650512695, Accuracy = 0.975000083447\n",
      "Training epochs #6180000: Batch Loss = 14784.861328, Accuracy = 0.71799993515\n",
      "Performance on test set: Training epochs #6180000, Batch Loss = 302.738586426, Accuracy = 0.991000056267\n",
      "Training epochs #6200000: Batch Loss = 1708.917236, Accuracy = 0.950999975204\n",
      "Performance on test set: Training epochs #6200000, Batch Loss = 370.101928711, Accuracy = 0.98299998045\n",
      "Training epochs #6220000: Batch Loss = 1938.609131, Accuracy = 0.955000042915\n",
      "Performance on test set: Training epochs #6220000, Batch Loss = 515.167419434, Accuracy = 0.975000023842\n",
      "Training epochs #6240000: Batch Loss = 994.769104, Accuracy = 0.97100007534\n",
      "Performance on test set: Training epochs #6240000, Batch Loss = 516.789550781, Accuracy = 0.975000023842\n",
      "Training epochs #6260000: Batch Loss = 12966.997070, Accuracy = 0.722000002861\n",
      "Performance on test set: Training epochs #6260000, Batch Loss = 295.451812744, Accuracy = 0.991000056267\n",
      "Training epochs #6280000: Batch Loss = 1375.905518, Accuracy = 0.95300000906\n",
      "Performance on test set: Training epochs #6280000, Batch Loss = 348.98638916, Accuracy = 0.986999988556\n",
      "Training epochs #6300000: Batch Loss = 1832.931152, Accuracy = 0.95400005579\n",
      "Performance on test set: Training epochs #6300000, Batch Loss = 459.681945801, Accuracy = 0.976999998093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #6320000: Batch Loss = 429.284210, Accuracy = 0.974000036716\n",
      "Performance on test set: Training epochs #6320000, Batch Loss = 486.864837646, Accuracy = 0.975000023842\n",
      "Training epochs #6340000: Batch Loss = 14488.637695, Accuracy = 0.709999978542\n",
      "Performance on test set: Training epochs #6340000, Batch Loss = 280.575622559, Accuracy = 0.991000056267\n",
      "Training epochs #6360000: Batch Loss = 1453.324219, Accuracy = 0.963000059128\n",
      "Performance on test set: Training epochs #6360000, Batch Loss = 333.952911377, Accuracy = 0.988000035286\n",
      "Training epochs #6380000: Batch Loss = 1493.553467, Accuracy = 0.958000063896\n",
      "Performance on test set: Training epochs #6380000, Batch Loss = 441.559051514, Accuracy = 0.980000078678\n",
      "Training epochs #6400000: Batch Loss = 1139.028442, Accuracy = 0.968000054359\n",
      "Performance on test set: Training epochs #6400000, Batch Loss = 437.644012451, Accuracy = 0.976000070572\n",
      "Training epochs #6420000: Batch Loss = 13697.005859, Accuracy = 0.708000004292\n",
      "Performance on test set: Training epochs #6420000, Batch Loss = 276.771270752, Accuracy = 0.991000056267\n",
      "Training epochs #6440000: Batch Loss = 1434.084961, Accuracy = 0.949000060558\n",
      "Performance on test set: Training epochs #6440000, Batch Loss = 310.534301758, Accuracy = 0.987999975681\n",
      "Training epochs #6460000: Batch Loss = 1441.025879, Accuracy = 0.962999999523\n",
      "Performance on test set: Training epochs #6460000, Batch Loss = 431.580932617, Accuracy = 0.980000138283\n",
      "Training epochs #6480000: Batch Loss = 685.509521, Accuracy = 0.976999998093\n",
      "Performance on test set: Training epochs #6480000, Batch Loss = 406.241027832, Accuracy = 0.980000078678\n",
      "Training epochs #6500000: Batch Loss = 10719.714844, Accuracy = 0.732000052929\n",
      "Performance on test set: Training epochs #6500000, Batch Loss = 261.734649658, Accuracy = 0.991000115871\n",
      "Training epochs #6520000: Batch Loss = 1804.534180, Accuracy = 0.942000031471\n",
      "Performance on test set: Training epochs #6520000, Batch Loss = 288.848114014, Accuracy = 0.990000128746\n",
      "Training epochs #6540000: Batch Loss = 1817.356567, Accuracy = 0.952000021935\n",
      "Performance on test set: Training epochs #6540000, Batch Loss = 401.512878418, Accuracy = 0.982000112534\n",
      "Training epochs #6560000: Batch Loss = 824.445923, Accuracy = 0.973999977112\n",
      "Performance on test set: Training epochs #6560000, Batch Loss = 379.923675537, Accuracy = 0.981999993324\n",
      "Training epochs #6580000: Batch Loss = 11718.761719, Accuracy = 0.736999988556\n",
      "Performance on test set: Training epochs #6580000, Batch Loss = 254.127380371, Accuracy = 0.991000056267\n",
      "Training epochs #6600000: Batch Loss = 1353.919800, Accuracy = 0.944999992847\n",
      "Performance on test set: Training epochs #6600000, Batch Loss = 278.857635498, Accuracy = 0.990000128746\n",
      "Training epochs #6620000: Batch Loss = 1491.288574, Accuracy = 0.963000059128\n",
      "Performance on test set: Training epochs #6620000, Batch Loss = 366.934906006, Accuracy = 0.982000052929\n",
      "Training epochs #6640000: Batch Loss = 719.950439, Accuracy = 0.972000062466\n",
      "Performance on test set: Training epochs #6640000, Batch Loss = 344.374023438, Accuracy = 0.981999993324\n",
      "Training epochs #6660000: Batch Loss = 10435.672852, Accuracy = 0.745999991894\n",
      "Performance on test set: Training epochs #6660000, Batch Loss = 251.572021484, Accuracy = 0.991000056267\n",
      "Training epochs #6680000: Batch Loss = 1341.990479, Accuracy = 0.947999954224\n",
      "Performance on test set: Training epochs #6680000, Batch Loss = 260.187011719, Accuracy = 0.990000128746\n",
      "Training epochs #6700000: Batch Loss = 1950.171143, Accuracy = 0.95400005579\n",
      "Performance on test set: Training epochs #6700000, Batch Loss = 334.459106445, Accuracy = 0.982000052929\n",
      "Training epochs #6720000: Batch Loss = 671.412537, Accuracy = 0.976999998093\n",
      "Performance on test set: Training epochs #6720000, Batch Loss = 327.260803223, Accuracy = 0.981999993324\n",
      "Training epochs #6740000: Batch Loss = 11140.205078, Accuracy = 0.741999983788\n",
      "Performance on test set: Training epochs #6740000, Batch Loss = 239.507049561, Accuracy = 0.991000056267\n",
      "Training epochs #6760000: Batch Loss = 1605.740356, Accuracy = 0.948000073433\n",
      "Performance on test set: Training epochs #6760000, Batch Loss = 251.31993103, Accuracy = 0.990000069141\n",
      "Training epochs #6780000: Batch Loss = 1402.706055, Accuracy = 0.957000017166\n",
      "Performance on test set: Training epochs #6780000, Batch Loss = 338.692077637, Accuracy = 0.982000112534\n",
      "Training epochs #6800000: Batch Loss = 672.644043, Accuracy = 0.979000031948\n",
      "Performance on test set: Training epochs #6800000, Batch Loss = 310.020324707, Accuracy = 0.982000052929\n",
      "Training epochs #6820000: Batch Loss = 10091.767578, Accuracy = 0.753999948502\n",
      "Performance on test set: Training epochs #6820000, Batch Loss = 232.177719116, Accuracy = 0.991000056267\n",
      "Training epochs #6840000: Batch Loss = 1180.074829, Accuracy = 0.955999970436\n",
      "Performance on test set: Training epochs #6840000, Batch Loss = 246.591339111, Accuracy = 0.990000069141\n",
      "Training epochs #6860000: Batch Loss = 1207.515869, Accuracy = 0.965000033379\n",
      "Performance on test set: Training epochs #6860000, Batch Loss = 285.867248535, Accuracy = 0.982000112534\n",
      "Training epochs #6880000: Batch Loss = 533.759827, Accuracy = 0.977999985218\n",
      "Performance on test set: Training epochs #6880000, Batch Loss = 271.695861816, Accuracy = 0.982000052929\n",
      "Training epochs #6900000: Batch Loss = 12183.222656, Accuracy = 0.75\n",
      "Performance on test set: Training epochs #6900000, Batch Loss = 222.153640747, Accuracy = 0.991000056267\n",
      "Training epochs #6920000: Batch Loss = 990.209534, Accuracy = 0.960000097752\n",
      "Performance on test set: Training epochs #6920000, Batch Loss = 251.405776978, Accuracy = 0.990000009537\n",
      "Training epochs #6940000: Batch Loss = 1655.989380, Accuracy = 0.954999983311\n",
      "Performance on test set: Training epochs #6940000, Batch Loss = 278.019836426, Accuracy = 0.982000052929\n",
      "Training epochs #6960000: Batch Loss = 411.705872, Accuracy = 0.978999972343\n",
      "Performance on test set: Training epochs #6960000, Batch Loss = 271.087219238, Accuracy = 0.982000052929\n",
      "Training epochs #6980000: Batch Loss = 9310.474609, Accuracy = 0.763999998569\n",
      "Performance on test set: Training epochs #6980000, Batch Loss = 229.184310913, Accuracy = 0.991000056267\n",
      "Training epochs #7000000: Batch Loss = 1219.502441, Accuracy = 0.955000042915\n",
      "Performance on test set: Training epochs #7000000, Batch Loss = 214.778564453, Accuracy = 0.990000128746\n",
      "Training epochs #7020000: Batch Loss = 1306.375977, Accuracy = 0.962000012398\n",
      "Performance on test set: Training epochs #7020000, Batch Loss = 242.051727295, Accuracy = 0.982000052929\n",
      "Training epochs #7040000: Batch Loss = 430.878357, Accuracy = 0.978000044823\n",
      "Performance on test set: Training epochs #7040000, Batch Loss = 229.826828003, Accuracy = 0.982000052929\n",
      "Training epochs #7060000: Batch Loss = 9293.236328, Accuracy = 0.764999985695\n",
      "Performance on test set: Training epochs #7060000, Batch Loss = 218.522033691, Accuracy = 0.991000056267\n",
      "Training epochs #7080000: Batch Loss = 913.857849, Accuracy = 0.958000004292\n",
      "Performance on test set: Training epochs #7080000, Batch Loss = 196.347961426, Accuracy = 0.990000128746\n",
      "Training epochs #7100000: Batch Loss = 1414.523682, Accuracy = 0.967000067234\n",
      "Performance on test set: Training epochs #7100000, Batch Loss = 236.056182861, Accuracy = 0.982000112534\n",
      "Training epochs #7120000: Batch Loss = 557.642517, Accuracy = 0.978999972343\n",
      "Performance on test set: Training epochs #7120000, Batch Loss = 217.23008728, Accuracy = 0.982000112534\n",
      "Training epochs #7140000: Batch Loss = 7874.259766, Accuracy = 0.771000027657\n",
      "Performance on test set: Training epochs #7140000, Batch Loss = 209.330932617, Accuracy = 0.991000056267\n",
      "Training epochs #7160000: Batch Loss = 931.035400, Accuracy = 0.958000004292\n",
      "Performance on test set: Training epochs #7160000, Batch Loss = 178.833465576, Accuracy = 0.990000128746\n",
      "Training epochs #7180000: Batch Loss = 1047.274658, Accuracy = 0.97100007534\n",
      "Performance on test set: Training epochs #7180000, Batch Loss = 220.001647949, Accuracy = 0.981999993324\n",
      "Training epochs #7200000: Batch Loss = 326.513092, Accuracy = 0.9849999547\n",
      "Performance on test set: Training epochs #7200000, Batch Loss = 205.041870117, Accuracy = 0.982000052929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #7220000: Batch Loss = 8557.779297, Accuracy = 0.761999964714\n",
      "Performance on test set: Training epochs #7220000, Batch Loss = 198.029174805, Accuracy = 0.991000056267\n",
      "Training epochs #7240000: Batch Loss = 958.284241, Accuracy = 0.964000046253\n",
      "Performance on test set: Training epochs #7240000, Batch Loss = 146.909683228, Accuracy = 0.990000009537\n",
      "Training epochs #7260000: Batch Loss = 1525.940063, Accuracy = 0.966000080109\n",
      "Performance on test set: Training epochs #7260000, Batch Loss = 174.691436768, Accuracy = 0.982000112534\n",
      "Training epochs #7280000: Batch Loss = 546.083069, Accuracy = 0.972000002861\n",
      "Performance on test set: Training epochs #7280000, Batch Loss = 169.248733521, Accuracy = 0.982000112534\n",
      "Training epochs #7300000: Batch Loss = 8421.355469, Accuracy = 0.754000008106\n",
      "Performance on test set: Training epochs #7300000, Batch Loss = 178.399993896, Accuracy = 0.991000056267\n",
      "Training epochs #7320000: Batch Loss = 830.794861, Accuracy = 0.97000002861\n",
      "Performance on test set: Training epochs #7320000, Batch Loss = 131.018386841, Accuracy = 0.990000128746\n",
      "Training epochs #7340000: Batch Loss = 879.219238, Accuracy = 0.973000049591\n",
      "Performance on test set: Training epochs #7340000, Batch Loss = 132.573577881, Accuracy = 0.981999993324\n",
      "Training epochs #7360000: Batch Loss = 662.160461, Accuracy = 0.975000023842\n",
      "Performance on test set: Training epochs #7360000, Batch Loss = 135.815658569, Accuracy = 0.982000112534\n",
      "Training epochs #7380000: Batch Loss = 6824.099609, Accuracy = 0.785000026226\n",
      "Performance on test set: Training epochs #7380000, Batch Loss = 179.927703857, Accuracy = 0.991000056267\n",
      "Training epochs #7400000: Batch Loss = 746.280151, Accuracy = 0.963999986649\n",
      "Performance on test set: Training epochs #7400000, Batch Loss = 119.031570435, Accuracy = 0.990000128746\n",
      "Training epochs #7420000: Batch Loss = 740.103882, Accuracy = 0.973000049591\n",
      "Performance on test set: Training epochs #7420000, Batch Loss = 128.233428955, Accuracy = 0.982000052929\n",
      "Training epochs #7440000: Batch Loss = 565.836670, Accuracy = 0.977999985218\n",
      "Performance on test set: Training epochs #7440000, Batch Loss = 113.425422668, Accuracy = 0.987999975681\n",
      "Training epochs #7460000: Batch Loss = 6583.616211, Accuracy = 0.794000029564\n",
      "Performance on test set: Training epochs #7460000, Batch Loss = 158.515213013, Accuracy = 0.991000056267\n",
      "Training epochs #7480000: Batch Loss = 684.439880, Accuracy = 0.965000033379\n",
      "Performance on test set: Training epochs #7480000, Batch Loss = 100.838378906, Accuracy = 0.990000009537\n",
      "Training epochs #7500000: Batch Loss = 1090.715698, Accuracy = 0.96899998188\n",
      "Performance on test set: Training epochs #7500000, Batch Loss = 115.211982727, Accuracy = 0.988000035286\n",
      "Training epochs #7520000: Batch Loss = 438.935516, Accuracy = 0.977999985218\n",
      "Performance on test set: Training epochs #7520000, Batch Loss = 121.367752075, Accuracy = 0.987999975681\n",
      "Training epochs #7540000: Batch Loss = 7232.369141, Accuracy = 0.782000005245\n",
      "Performance on test set: Training epochs #7540000, Batch Loss = 137.766998291, Accuracy = 0.991000056267\n",
      "Training epochs #7560000: Batch Loss = 773.130005, Accuracy = 0.964000046253\n",
      "Performance on test set: Training epochs #7560000, Batch Loss = 87.3500671387, Accuracy = 0.990000069141\n",
      "Training epochs #7580000: Batch Loss = 885.974854, Accuracy = 0.967000007629\n",
      "Performance on test set: Training epochs #7580000, Batch Loss = 94.916885376, Accuracy = 0.988000094891\n",
      "Training epochs #7600000: Batch Loss = 330.395081, Accuracy = 0.986999988556\n",
      "Performance on test set: Training epochs #7600000, Batch Loss = 93.6395645142, Accuracy = 0.988000035286\n",
      "Training epochs #7620000: Batch Loss = 7514.898438, Accuracy = 0.778999984264\n",
      "Performance on test set: Training epochs #7620000, Batch Loss = 123.370391846, Accuracy = 0.991000056267\n",
      "Training epochs #7640000: Batch Loss = 659.602966, Accuracy = 0.967999994755\n",
      "Performance on test set: Training epochs #7640000, Batch Loss = 86.4529418945, Accuracy = 0.990000009537\n",
      "Training epochs #7660000: Batch Loss = 949.971802, Accuracy = 0.972999989986\n",
      "Performance on test set: Training epochs #7660000, Batch Loss = 97.7494049072, Accuracy = 0.988000035286\n",
      "Training epochs #7680000: Batch Loss = 370.312897, Accuracy = 0.984000086784\n",
      "Performance on test set: Training epochs #7680000, Batch Loss = 70.5159454346, Accuracy = 0.988000035286\n",
      "Training epochs #7700000: Batch Loss = 6879.340820, Accuracy = 0.802000045776\n",
      "Performance on test set: Training epochs #7700000, Batch Loss = 105.224517822, Accuracy = 0.991000056267\n",
      "Training epochs #7720000: Batch Loss = 901.206299, Accuracy = 0.96899998188\n",
      "Performance on test set: Training epochs #7720000, Batch Loss = 83.7224960327, Accuracy = 0.990000009537\n",
      "Training epochs #7740000: Batch Loss = 485.813110, Accuracy = 0.985000014305\n",
      "Performance on test set: Training epochs #7740000, Batch Loss = 103.565750122, Accuracy = 0.988000035286\n",
      "Training epochs #7760000: Batch Loss = 350.713287, Accuracy = 0.982000052929\n",
      "Performance on test set: Training epochs #7760000, Batch Loss = 83.9552078247, Accuracy = 0.988000035286\n",
      "Training epochs #7780000: Batch Loss = 6557.067383, Accuracy = 0.796000063419\n",
      "Performance on test set: Training epochs #7780000, Batch Loss = 86.1893997192, Accuracy = 0.991000056267\n",
      "Training epochs #7800000: Batch Loss = 636.509827, Accuracy = 0.966000139713\n",
      "Performance on test set: Training epochs #7800000, Batch Loss = 50.6949539185, Accuracy = 0.990000069141\n",
      "Training epochs #7820000: Batch Loss = 928.525635, Accuracy = 0.967999994755\n",
      "Performance on test set: Training epochs #7820000, Batch Loss = 103.023498535, Accuracy = 0.988000094891\n",
      "Training epochs #7840000: Batch Loss = 422.198914, Accuracy = 0.981000006199\n",
      "Performance on test set: Training epochs #7840000, Batch Loss = 72.1725769043, Accuracy = 0.988000035286\n",
      "Training epochs #7860000: Batch Loss = 5809.799805, Accuracy = 0.801999986172\n",
      "Performance on test set: Training epochs #7860000, Batch Loss = 89.3949890137, Accuracy = 0.990999996662\n",
      "Training epochs #7880000: Batch Loss = 736.214172, Accuracy = 0.967000007629\n",
      "Performance on test set: Training epochs #7880000, Batch Loss = 49.5624389648, Accuracy = 0.996000051498\n",
      "Training epochs #7900000: Batch Loss = 968.752991, Accuracy = 0.97000002861\n",
      "Performance on test set: Training epochs #7900000, Batch Loss = 81.8905487061, Accuracy = 0.988000035286\n",
      "Training epochs #7920000: Batch Loss = 314.453186, Accuracy = 0.986999988556\n",
      "Performance on test set: Training epochs #7920000, Batch Loss = 55.2805252075, Accuracy = 0.988000035286\n",
      "Training epochs #7940000: Batch Loss = 4553.691895, Accuracy = 0.823000013828\n",
      "Performance on test set: Training epochs #7940000, Batch Loss = 74.6176376343, Accuracy = 0.991000056267\n",
      "Training epochs #7960000: Batch Loss = 758.682800, Accuracy = 0.962999999523\n",
      "Performance on test set: Training epochs #7960000, Batch Loss = 43.3240203857, Accuracy = 0.996000051498\n",
      "Training epochs #7980000: Batch Loss = 646.545715, Accuracy = 0.973000049591\n",
      "Performance on test set: Training epochs #7980000, Batch Loss = 59.0824623108, Accuracy = 0.993000030518\n",
      "Training epochs #8000000: Batch Loss = 209.059814, Accuracy = 0.990000009537\n",
      "Performance on test set: Training epochs #8000000, Batch Loss = 51.6694793701, Accuracy = 0.994000077248\n",
      "Training epochs #8020000: Batch Loss = 5346.036133, Accuracy = 0.81400001049\n",
      "Performance on test set: Training epochs #8020000, Batch Loss = 71.6991424561, Accuracy = 0.991000056267\n",
      "Training epochs #8040000: Batch Loss = 716.926147, Accuracy = 0.962999999523\n",
      "Performance on test set: Training epochs #8040000, Batch Loss = 41.1400337219, Accuracy = 0.996000051498\n",
      "Training epochs #8060000: Batch Loss = 673.053345, Accuracy = 0.976000070572\n",
      "Performance on test set: Training epochs #8060000, Batch Loss = 52.9031600952, Accuracy = 0.996000051498\n",
      "Training epochs #8080000: Batch Loss = 360.085999, Accuracy = 0.983000040054\n",
      "Performance on test set: Training epochs #8080000, Batch Loss = 42.1273727417, Accuracy = 0.996000051498\n",
      "Training epochs #8100000: Batch Loss = 5577.621094, Accuracy = 0.80999994278\n",
      "Performance on test set: Training epochs #8100000, Batch Loss = 68.2241668701, Accuracy = 0.991000056267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #8120000: Batch Loss = 590.350830, Accuracy = 0.977000057697\n",
      "Performance on test set: Training epochs #8120000, Batch Loss = 37.2717285156, Accuracy = 0.995999991894\n",
      "Training epochs #8140000: Batch Loss = 705.684692, Accuracy = 0.974999904633\n",
      "Performance on test set: Training epochs #8140000, Batch Loss = 45.6103210449, Accuracy = 0.996000051498\n",
      "Training epochs #8160000: Batch Loss = 357.524414, Accuracy = 0.984999895096\n",
      "Performance on test set: Training epochs #8160000, Batch Loss = 37.8029785156, Accuracy = 0.996000051498\n",
      "Training epochs #8180000: Batch Loss = 5286.364258, Accuracy = 0.822000026703\n",
      "Performance on test set: Training epochs #8180000, Batch Loss = 60.127948761, Accuracy = 0.991000115871\n",
      "Training epochs #8200000: Batch Loss = 438.949280, Accuracy = 0.973000049591\n",
      "Performance on test set: Training epochs #8200000, Batch Loss = 39.6771774292, Accuracy = 0.996000051498\n",
      "Training epochs #8220000: Batch Loss = 486.057709, Accuracy = 0.980000019073\n",
      "Performance on test set: Training epochs #8220000, Batch Loss = 49.2693099976, Accuracy = 0.993000030518\n",
      "Training epochs #8240000: Batch Loss = 129.239014, Accuracy = 0.988999962807\n",
      "Performance on test set: Training epochs #8240000, Batch Loss = 37.4159698486, Accuracy = 0.996000051498\n",
      "Training epochs #8260000: Batch Loss = 5229.133789, Accuracy = 0.811000049114\n",
      "Performance on test set: Training epochs #8260000, Batch Loss = 51.7529792786, Accuracy = 0.991000056267\n",
      "Training epochs #8280000: Batch Loss = 515.213379, Accuracy = 0.972000002861\n",
      "Performance on test set: Training epochs #8280000, Batch Loss = 33.5474472046, Accuracy = 0.997000098228\n",
      "Training epochs #8300000: Batch Loss = 386.631927, Accuracy = 0.978000044823\n",
      "Performance on test set: Training epochs #8300000, Batch Loss = 47.1414642334, Accuracy = 0.996000051498\n",
      "Training epochs #8320000: Batch Loss = 162.460129, Accuracy = 0.990000009537\n",
      "Performance on test set: Training epochs #8320000, Batch Loss = 37.528591156, Accuracy = 0.993999958038\n",
      "Training epochs #8340000: Batch Loss = 4690.884277, Accuracy = 0.807999968529\n",
      "Performance on test set: Training epochs #8340000, Batch Loss = 48.2375640869, Accuracy = 0.991000056267\n",
      "Training epochs #8360000: Batch Loss = 600.715088, Accuracy = 0.970999956131\n",
      "Performance on test set: Training epochs #8360000, Batch Loss = 33.1175613403, Accuracy = 0.996999979019\n",
      "Training epochs #8380000: Batch Loss = 303.654694, Accuracy = 0.981999993324\n",
      "Performance on test set: Training epochs #8380000, Batch Loss = 44.1914482117, Accuracy = 0.996000051498\n",
      "Training epochs #8400000: Batch Loss = 194.965881, Accuracy = 0.988000035286\n",
      "Performance on test set: Training epochs #8400000, Batch Loss = 34.1265106201, Accuracy = 0.996000051498\n",
      "Training epochs #8420000: Batch Loss = 4249.655273, Accuracy = 0.831000030041\n",
      "Performance on test set: Training epochs #8420000, Batch Loss = 45.6174087524, Accuracy = 0.991000056267\n",
      "Training epochs #8440000: Batch Loss = 151.611862, Accuracy = 0.983999967575\n",
      "Performance on test set: Training epochs #8440000, Batch Loss = 35.40650177, Accuracy = 0.996999979019\n",
      "Training epochs #8460000: Batch Loss = 501.710083, Accuracy = 0.978999972343\n",
      "Performance on test set: Training epochs #8460000, Batch Loss = 40.40807724, Accuracy = 0.996000051498\n",
      "Training epochs #8480000: Batch Loss = 97.479233, Accuracy = 0.991999983788\n",
      "Performance on test set: Training epochs #8480000, Batch Loss = 31.9608726501, Accuracy = 0.996000051498\n",
      "Training epochs #8500000: Batch Loss = 4250.636230, Accuracy = 0.838999986649\n",
      "Performance on test set: Training epochs #8500000, Batch Loss = 40.9009246826, Accuracy = 0.996999979019\n",
      "Training epochs #8520000: Batch Loss = 325.495850, Accuracy = 0.976999998093\n",
      "Performance on test set: Training epochs #8520000, Batch Loss = 29.7050819397, Accuracy = 0.997000098228\n",
      "Training epochs #8540000: Batch Loss = 619.256836, Accuracy = 0.979000091553\n",
      "Performance on test set: Training epochs #8540000, Batch Loss = 37.0886726379, Accuracy = 0.996000051498\n",
      "Training epochs #8560000: Batch Loss = 158.573730, Accuracy = 0.989000022411\n",
      "Performance on test set: Training epochs #8560000, Batch Loss = 32.0345420837, Accuracy = 0.996000051498\n",
      "Training epochs #8580000: Batch Loss = 4040.364990, Accuracy = 0.835000097752\n",
      "Performance on test set: Training epochs #8580000, Batch Loss = 37.68542099, Accuracy = 0.996999979019\n",
      "Training epochs #8600000: Batch Loss = 551.791382, Accuracy = 0.972000062466\n",
      "Performance on test set: Training epochs #8600000, Batch Loss = 25.6280879974, Accuracy = 0.996999979019\n",
      "Training epochs #8620000: Batch Loss = 524.928528, Accuracy = 0.973000049591\n",
      "Performance on test set: Training epochs #8620000, Batch Loss = 32.7301521301, Accuracy = 0.996000051498\n",
      "Training epochs #8640000: Batch Loss = 228.326019, Accuracy = 0.986000061035\n",
      "Performance on test set: Training epochs #8640000, Batch Loss = 24.1496486664, Accuracy = 0.995999991894\n",
      "Training epochs #8660000: Batch Loss = 3857.858887, Accuracy = 0.839999973774\n",
      "Performance on test set: Training epochs #8660000, Batch Loss = 31.141582489, Accuracy = 0.996999979019\n",
      "Training epochs #8680000: Batch Loss = 269.384399, Accuracy = 0.976999998093\n",
      "Performance on test set: Training epochs #8680000, Batch Loss = 20.7349815369, Accuracy = 0.997000098228\n",
      "Training epochs #8700000: Batch Loss = 633.109619, Accuracy = 0.978999972343\n",
      "Performance on test set: Training epochs #8700000, Batch Loss = 27.6881523132, Accuracy = 0.996000051498\n",
      "Training epochs #8720000: Batch Loss = 183.436020, Accuracy = 0.983999967575\n",
      "Performance on test set: Training epochs #8720000, Batch Loss = 21.8836746216, Accuracy = 0.996000051498\n",
      "Training epochs #8740000: Batch Loss = 4359.298828, Accuracy = 0.839999973774\n",
      "Performance on test set: Training epochs #8740000, Batch Loss = 31.905921936, Accuracy = 0.996999979019\n",
      "Training epochs #8760000: Batch Loss = 271.279510, Accuracy = 0.977000057697\n",
      "Performance on test set: Training epochs #8760000, Batch Loss = 21.7744827271, Accuracy = 0.996999979019\n",
      "Training epochs #8780000: Batch Loss = 378.767792, Accuracy = 0.983999967575\n",
      "Performance on test set: Training epochs #8780000, Batch Loss = 29.8171634674, Accuracy = 0.996000051498\n",
      "Training epochs #8800000: Batch Loss = 165.383377, Accuracy = 0.991000056267\n",
      "Performance on test set: Training epochs #8800000, Batch Loss = 23.1072978973, Accuracy = 0.995999991894\n",
      "Training epochs #8820000: Batch Loss = 3313.112549, Accuracy = 0.856000006199\n",
      "Performance on test set: Training epochs #8820000, Batch Loss = 28.2968864441, Accuracy = 0.996999979019\n",
      "Training epochs #8840000: Batch Loss = 395.504089, Accuracy = 0.975000083447\n",
      "Performance on test set: Training epochs #8840000, Batch Loss = 21.5978736877, Accuracy = 0.996999979019\n",
      "Training epochs #8860000: Batch Loss = 279.244812, Accuracy = 0.982000052929\n",
      "Performance on test set: Training epochs #8860000, Batch Loss = 23.9171085358, Accuracy = 0.995999991894\n",
      "Training epochs #8880000: Batch Loss = 122.913879, Accuracy = 0.983999967575\n",
      "Performance on test set: Training epochs #8880000, Batch Loss = 15.5446310043, Accuracy = 0.995999991894\n",
      "Training epochs #8900000: Batch Loss = 3817.054443, Accuracy = 0.857000052929\n",
      "Performance on test set: Training epochs #8900000, Batch Loss = 27.3171386719, Accuracy = 0.996999979019\n",
      "Training epochs #8920000: Batch Loss = 303.873840, Accuracy = 0.976000010967\n",
      "Performance on test set: Training epochs #8920000, Batch Loss = 21.2912998199, Accuracy = 0.997000098228\n",
      "Training epochs #8940000: Batch Loss = 492.489532, Accuracy = 0.979000031948\n",
      "Performance on test set: Training epochs #8940000, Batch Loss = 23.897939682, Accuracy = 0.996000051498\n",
      "Training epochs #8960000: Batch Loss = 107.729462, Accuracy = 0.990000069141\n",
      "Performance on test set: Training epochs #8960000, Batch Loss = 18.7064704895, Accuracy = 0.995999991894\n",
      "Training epochs #8980000: Batch Loss = 2915.366211, Accuracy = 0.861000001431\n",
      "Performance on test set: Training epochs #8980000, Batch Loss = 24.851688385, Accuracy = 0.997000098228\n",
      "Training epochs #9000000: Batch Loss = 226.459808, Accuracy = 0.983999967575\n",
      "Performance on test set: Training epochs #9000000, Batch Loss = 19.3997764587, Accuracy = 0.997000098228\n",
      "Training epochs #9020000: Batch Loss = 374.069641, Accuracy = 0.978000044823\n",
      "Performance on test set: Training epochs #9020000, Batch Loss = 25.4392471313, Accuracy = 0.995999991894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #9040000: Batch Loss = 72.640762, Accuracy = 0.992000043392\n",
      "Performance on test set: Training epochs #9040000, Batch Loss = 15.368268013, Accuracy = 0.996000051498\n",
      "Training epochs #9060000: Batch Loss = 3076.868164, Accuracy = 0.86000007391\n",
      "Performance on test set: Training epochs #9060000, Batch Loss = 19.7726840973, Accuracy = 0.996999979019\n",
      "Training epochs #9080000: Batch Loss = 244.463440, Accuracy = 0.976000010967\n",
      "Performance on test set: Training epochs #9080000, Batch Loss = 12.3637561798, Accuracy = 0.997000098228\n",
      "Training epochs #9100000: Batch Loss = 367.118225, Accuracy = 0.9849999547\n",
      "Performance on test set: Training epochs #9100000, Batch Loss = 20.285068512, Accuracy = 0.996000051498\n",
      "Training epochs #9120000: Batch Loss = 121.522278, Accuracy = 0.989999949932\n",
      "Performance on test set: Training epochs #9120000, Batch Loss = 15.8220405579, Accuracy = 0.996000051498\n",
      "Training epochs #9140000: Batch Loss = 3146.559570, Accuracy = 0.860999941826\n",
      "Performance on test set: Training epochs #9140000, Batch Loss = 16.2927799225, Accuracy = 0.997000098228\n",
      "Training epochs #9160000: Batch Loss = 317.097565, Accuracy = 0.970999956131\n",
      "Performance on test set: Training epochs #9160000, Batch Loss = 10.5959062576, Accuracy = 0.996999979019\n",
      "Training epochs #9180000: Batch Loss = 256.964142, Accuracy = 0.98500007391\n",
      "Performance on test set: Training epochs #9180000, Batch Loss = 15.413225174, Accuracy = 0.996000051498\n",
      "Training epochs #9200000: Batch Loss = 62.176605, Accuracy = 0.992000043392\n",
      "Performance on test set: Training epochs #9200000, Batch Loss = 10.8762102127, Accuracy = 0.996000051498\n",
      "Training epochs #9220000: Batch Loss = 3248.955811, Accuracy = 0.864000022411\n",
      "Performance on test set: Training epochs #9220000, Batch Loss = 13.7107801437, Accuracy = 0.996999979019\n",
      "Training epochs #9240000: Batch Loss = 133.947662, Accuracy = 0.988000035286\n",
      "Performance on test set: Training epochs #9240000, Batch Loss = 8.83693981171, Accuracy = 0.996999979019\n",
      "Training epochs #9260000: Batch Loss = 252.375702, Accuracy = 0.990000069141\n",
      "Performance on test set: Training epochs #9260000, Batch Loss = 12.6405067444, Accuracy = 0.996000051498\n",
      "Training epochs #9280000: Batch Loss = 133.736542, Accuracy = 0.986000061035\n",
      "Performance on test set: Training epochs #9280000, Batch Loss = 9.7211227417, Accuracy = 0.996000051498\n",
      "Training epochs #9300000: Batch Loss = 2915.671631, Accuracy = 0.866999983788\n",
      "Performance on test set: Training epochs #9300000, Batch Loss = 16.8333816528, Accuracy = 0.996999979019\n",
      "Training epochs #9320000: Batch Loss = 293.707031, Accuracy = 0.973000049591\n",
      "Performance on test set: Training epochs #9320000, Batch Loss = 4.49887514114, Accuracy = 0.996999979019\n",
      "Training epochs #9340000: Batch Loss = 274.704498, Accuracy = 0.986000061035\n",
      "Performance on test set: Training epochs #9340000, Batch Loss = 9.14131069183, Accuracy = 0.996999979019\n",
      "Training epochs #9360000: Batch Loss = 120.909088, Accuracy = 0.990000009537\n",
      "Performance on test set: Training epochs #9360000, Batch Loss = 7.56485176086, Accuracy = 0.996000051498\n",
      "Training epochs #9380000: Batch Loss = 2525.321533, Accuracy = 0.884000062943\n",
      "Performance on test set: Training epochs #9380000, Batch Loss = 11.9849739075, Accuracy = 0.996999979019\n",
      "Training epochs #9400000: Batch Loss = 238.148407, Accuracy = 0.978000044823\n",
      "Performance on test set: Training epochs #9400000, Batch Loss = 4.55728149414, Accuracy = 0.996999979019\n",
      "Training epochs #9420000: Batch Loss = 178.663803, Accuracy = 0.986000061035\n",
      "Performance on test set: Training epochs #9420000, Batch Loss = 10.1405448914, Accuracy = 0.996999979019\n",
      "Training epochs #9440000: Batch Loss = 219.205017, Accuracy = 0.992000043392\n",
      "Performance on test set: Training epochs #9440000, Batch Loss = 5.41194152832, Accuracy = 0.996000051498\n",
      "Training epochs #9460000: Batch Loss = 2420.301270, Accuracy = 0.874000072479\n",
      "Performance on test set: Training epochs #9460000, Batch Loss = 5.7993721962, Accuracy = 0.996999979019\n",
      "Training epochs #9480000: Batch Loss = 82.379753, Accuracy = 0.992000043392\n",
      "Performance on test set: Training epochs #9480000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #9500000: Batch Loss = 248.572372, Accuracy = 0.983999967575\n",
      "Performance on test set: Training epochs #9500000, Batch Loss = 4.17254304886, Accuracy = 0.996999979019\n",
      "Training epochs #9520000: Batch Loss = 186.305527, Accuracy = 0.990999937057\n",
      "Performance on test set: Training epochs #9520000, Batch Loss = 1.01444530487, Accuracy = 0.999000072479\n",
      "Training epochs #9540000: Batch Loss = 2164.837158, Accuracy = 0.87600004673\n",
      "Performance on test set: Training epochs #9540000, Batch Loss = 9.84568977356, Accuracy = 0.996999979019\n",
      "Training epochs #9560000: Batch Loss = 119.341629, Accuracy = 0.984000086784\n",
      "Performance on test set: Training epochs #9560000, Batch Loss = 1.96796500683, Accuracy = 0.996999979019\n",
      "Training epochs #9580000: Batch Loss = 315.268066, Accuracy = 0.98199993372\n",
      "Performance on test set: Training epochs #9580000, Batch Loss = 1.80795717239, Accuracy = 0.996999979019\n",
      "Training epochs #9600000: Batch Loss = 137.655991, Accuracy = 0.991000056267\n",
      "Performance on test set: Training epochs #9600000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #9620000: Batch Loss = 2262.672607, Accuracy = 0.881000041962\n",
      "Performance on test set: Training epochs #9620000, Batch Loss = 7.93375825882, Accuracy = 0.996999979019\n",
      "Training epochs #9640000: Batch Loss = 150.076935, Accuracy = 0.981000006199\n",
      "Performance on test set: Training epochs #9640000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #9660000: Batch Loss = 274.666229, Accuracy = 0.9849999547\n",
      "Performance on test set: Training epochs #9660000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #9680000: Batch Loss = 84.881607, Accuracy = 0.991999983788\n",
      "Performance on test set: Training epochs #9680000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #9700000: Batch Loss = 2059.576904, Accuracy = 0.884999990463\n",
      "Performance on test set: Training epochs #9700000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #9720000: Batch Loss = 240.789459, Accuracy = 0.979000031948\n",
      "Performance on test set: Training epochs #9720000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #9740000: Batch Loss = 171.839478, Accuracy = 0.986000061035\n",
      "Performance on test set: Training epochs #9740000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #9760000: Batch Loss = 78.720917, Accuracy = 0.992000043392\n",
      "Performance on test set: Training epochs #9760000, Batch Loss = 0.212730482221, Accuracy = 0.999000072479\n",
      "Training epochs #9780000: Batch Loss = 2024.311523, Accuracy = 0.881000041962\n",
      "Performance on test set: Training epochs #9780000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #9800000: Batch Loss = 205.037537, Accuracy = 0.98400002718\n",
      "Performance on test set: Training epochs #9800000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #9820000: Batch Loss = 337.526398, Accuracy = 0.985999941826\n",
      "Performance on test set: Training epochs #9820000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #9840000: Batch Loss = 58.890282, Accuracy = 0.995999932289\n",
      "Performance on test set: Training epochs #9840000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #9860000: Batch Loss = 1541.446045, Accuracy = 0.893000006676\n",
      "Performance on test set: Training epochs #9860000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #9880000: Batch Loss = 175.742569, Accuracy = 0.981999993324\n",
      "Performance on test set: Training epochs #9880000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #9900000: Batch Loss = 82.057739, Accuracy = 0.989000022411\n",
      "Performance on test set: Training epochs #9900000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #9920000: Batch Loss = 95.410042, Accuracy = 0.993000030518\n",
      "Performance on test set: Training epochs #9920000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #9940000: Batch Loss = 2141.061035, Accuracy = 0.877999961376\n",
      "Performance on test set: Training epochs #9940000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #9960000: Batch Loss = 117.724670, Accuracy = 0.990999996662\n",
      "Performance on test set: Training epochs #9960000, Batch Loss = 0.0, Accuracy = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #9980000: Batch Loss = 297.367767, Accuracy = 0.984000086784\n",
      "Performance on test set: Training epochs #9980000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10000000: Batch Loss = 57.666191, Accuracy = 0.994000077248\n",
      "Performance on test set: Training epochs #10000000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10020000: Batch Loss = 1666.166260, Accuracy = 0.895000040531\n",
      "Performance on test set: Training epochs #10020000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #10040000: Batch Loss = 201.848679, Accuracy = 0.980000019073\n",
      "Performance on test set: Training epochs #10040000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10060000: Batch Loss = 276.069183, Accuracy = 0.98400002718\n",
      "Performance on test set: Training epochs #10060000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10080000: Batch Loss = 81.292839, Accuracy = 0.991999983788\n",
      "Performance on test set: Training epochs #10080000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10100000: Batch Loss = 1489.321655, Accuracy = 0.895000040531\n",
      "Performance on test set: Training epochs #10100000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #10120000: Batch Loss = 73.240273, Accuracy = 0.988000094891\n",
      "Performance on test set: Training epochs #10120000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10140000: Batch Loss = 200.404388, Accuracy = 0.989000022411\n",
      "Performance on test set: Training epochs #10140000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10160000: Batch Loss = 83.129570, Accuracy = 0.991000056267\n",
      "Performance on test set: Training epochs #10160000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10180000: Batch Loss = 1539.492676, Accuracy = 0.888999998569\n",
      "Performance on test set: Training epochs #10180000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10200000: Batch Loss = 80.744629, Accuracy = 0.992000043392\n",
      "Performance on test set: Training epochs #10200000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10220000: Batch Loss = 108.336823, Accuracy = 0.988000035286\n",
      "Performance on test set: Training epochs #10220000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #10240000: Batch Loss = 56.000145, Accuracy = 0.996000051498\n",
      "Performance on test set: Training epochs #10240000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10260000: Batch Loss = 1577.399780, Accuracy = 0.911000013351\n",
      "Performance on test set: Training epochs #10260000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10280000: Batch Loss = 147.051697, Accuracy = 0.980999946594\n",
      "Performance on test set: Training epochs #10280000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #10300000: Batch Loss = 179.588776, Accuracy = 0.983000040054\n",
      "Performance on test set: Training epochs #10300000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10320000: Batch Loss = 20.475206, Accuracy = 0.996000051498\n",
      "Performance on test set: Training epochs #10320000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10340000: Batch Loss = 1670.383057, Accuracy = 0.897000014782\n",
      "Performance on test set: Training epochs #10340000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10360000: Batch Loss = 146.682251, Accuracy = 0.98299998045\n",
      "Performance on test set: Training epochs #10360000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10380000: Batch Loss = 105.251373, Accuracy = 0.988000035286\n",
      "Performance on test set: Training epochs #10380000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #10400000: Batch Loss = 43.950260, Accuracy = 0.991999983788\n",
      "Performance on test set: Training epochs #10400000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10420000: Batch Loss = 995.380371, Accuracy = 0.907000005245\n",
      "Performance on test set: Training epochs #10420000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10440000: Batch Loss = 163.981873, Accuracy = 0.98400002718\n",
      "Performance on test set: Training epochs #10440000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10460000: Batch Loss = 152.600449, Accuracy = 0.985999941826\n",
      "Performance on test set: Training epochs #10460000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10480000: Batch Loss = 20.549772, Accuracy = 0.996000051498\n",
      "Performance on test set: Training epochs #10480000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10500000: Batch Loss = 967.799805, Accuracy = 0.921000003815\n",
      "Performance on test set: Training epochs #10500000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10520000: Batch Loss = 157.932007, Accuracy = 0.983000040054\n",
      "Performance on test set: Training epochs #10520000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #10540000: Batch Loss = 231.832092, Accuracy = 0.98400002718\n",
      "Performance on test set: Training epochs #10540000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10560000: Batch Loss = 54.945225, Accuracy = 0.993000030518\n",
      "Performance on test set: Training epochs #10560000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10580000: Batch Loss = 1304.977661, Accuracy = 0.919000029564\n",
      "Performance on test set: Training epochs #10580000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10600000: Batch Loss = 79.419815, Accuracy = 0.990999996662\n",
      "Performance on test set: Training epochs #10600000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10620000: Batch Loss = 122.314819, Accuracy = 0.993000030518\n",
      "Performance on test set: Training epochs #10620000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10640000: Batch Loss = 14.840651, Accuracy = 0.994000017643\n",
      "Performance on test set: Training epochs #10640000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #10660000: Batch Loss = 1231.319336, Accuracy = 0.893999993801\n",
      "Performance on test set: Training epochs #10660000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #10680000: Batch Loss = 88.910721, Accuracy = 0.988000035286\n",
      "Performance on test set: Training epochs #10680000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10700000: Batch Loss = 175.689621, Accuracy = 0.989999949932\n",
      "Performance on test set: Training epochs #10700000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10720000: Batch Loss = 46.865429, Accuracy = 0.995000064373\n",
      "Performance on test set: Training epochs #10720000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10740000: Batch Loss = 1322.384033, Accuracy = 0.913999974728\n",
      "Performance on test set: Training epochs #10740000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10760000: Batch Loss = 46.469646, Accuracy = 0.989000082016\n",
      "Performance on test set: Training epochs #10760000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10780000: Batch Loss = 74.872337, Accuracy = 0.983999967575\n",
      "Performance on test set: Training epochs #10780000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10800000: Batch Loss = 46.724339, Accuracy = 0.995999991894\n",
      "Performance on test set: Training epochs #10800000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10820000: Batch Loss = 1144.548950, Accuracy = 0.912000060081\n",
      "Performance on test set: Training epochs #10820000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10840000: Batch Loss = 129.755890, Accuracy = 0.986000061035\n",
      "Performance on test set: Training epochs #10840000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10860000: Batch Loss = 197.832428, Accuracy = 0.981000006199\n",
      "Performance on test set: Training epochs #10860000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10880000: Batch Loss = 54.495392, Accuracy = 0.994000077248\n",
      "Performance on test set: Training epochs #10880000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10900000: Batch Loss = 1143.629639, Accuracy = 0.919000089169\n",
      "Performance on test set: Training epochs #10900000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #10920000: Batch Loss = 26.851233, Accuracy = 0.989000082016\n",
      "Performance on test set: Training epochs #10920000, Batch Loss = 0.0, Accuracy = 1.00000011921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #10940000: Batch Loss = 79.747658, Accuracy = 0.991000056267\n",
      "Performance on test set: Training epochs #10940000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10960000: Batch Loss = 20.809303, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #10960000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #10980000: Batch Loss = 1114.729614, Accuracy = 0.917999982834\n",
      "Performance on test set: Training epochs #10980000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11000000: Batch Loss = 15.921213, Accuracy = 0.994000077248\n",
      "Performance on test set: Training epochs #11000000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11020000: Batch Loss = 217.133728, Accuracy = 0.986999988556\n",
      "Performance on test set: Training epochs #11020000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11040000: Batch Loss = 18.471336, Accuracy = 0.997000038624\n",
      "Performance on test set: Training epochs #11040000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11060000: Batch Loss = 688.899231, Accuracy = 0.938000023365\n",
      "Performance on test set: Training epochs #11060000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11080000: Batch Loss = 64.331375, Accuracy = 0.989000082016\n",
      "Performance on test set: Training epochs #11080000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11100000: Batch Loss = 67.573448, Accuracy = 0.990999996662\n",
      "Performance on test set: Training epochs #11100000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11120000: Batch Loss = 30.231480, Accuracy = 0.995000064373\n",
      "Performance on test set: Training epochs #11120000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11140000: Batch Loss = 756.396301, Accuracy = 0.925000011921\n",
      "Performance on test set: Training epochs #11140000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11160000: Batch Loss = 119.642532, Accuracy = 0.985000014305\n",
      "Performance on test set: Training epochs #11160000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11180000: Batch Loss = 78.451828, Accuracy = 0.989000082016\n",
      "Performance on test set: Training epochs #11180000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11200000: Batch Loss = 12.029068, Accuracy = 0.998000144958\n",
      "Performance on test set: Training epochs #11200000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11220000: Batch Loss = 925.631226, Accuracy = 0.925999999046\n",
      "Performance on test set: Training epochs #11220000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11240000: Batch Loss = 97.184296, Accuracy = 0.986000001431\n",
      "Performance on test set: Training epochs #11240000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11260000: Batch Loss = 40.392143, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #11260000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11280000: Batch Loss = 11.969572, Accuracy = 0.996000051498\n",
      "Performance on test set: Training epochs #11280000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11300000: Batch Loss = 794.312073, Accuracy = 0.924000024796\n",
      "Performance on test set: Training epochs #11300000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11320000: Batch Loss = 61.209759, Accuracy = 0.991000056267\n",
      "Performance on test set: Training epochs #11320000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11340000: Batch Loss = 95.391411, Accuracy = 0.990999996662\n",
      "Performance on test set: Training epochs #11340000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11360000: Batch Loss = 28.448210, Accuracy = 0.995999991894\n",
      "Performance on test set: Training epochs #11360000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11380000: Batch Loss = 603.050720, Accuracy = 0.930000007153\n",
      "Performance on test set: Training epochs #11380000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #11400000: Batch Loss = 55.918404, Accuracy = 0.991000056267\n",
      "Performance on test set: Training epochs #11400000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11420000: Batch Loss = 82.350861, Accuracy = 0.990000009537\n",
      "Performance on test set: Training epochs #11420000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11440000: Batch Loss = 22.944990, Accuracy = 0.996000051498\n",
      "Performance on test set: Training epochs #11440000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11460000: Batch Loss = 515.582214, Accuracy = 0.945999979973\n",
      "Performance on test set: Training epochs #11460000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11480000: Batch Loss = 51.463528, Accuracy = 0.990999996662\n",
      "Performance on test set: Training epochs #11480000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11500000: Batch Loss = 37.377079, Accuracy = 0.994000077248\n",
      "Performance on test set: Training epochs #11500000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #11520000: Batch Loss = 17.655188, Accuracy = 0.997000098228\n",
      "Performance on test set: Training epochs #11520000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11540000: Batch Loss = 670.637146, Accuracy = 0.93700003624\n",
      "Performance on test set: Training epochs #11540000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11560000: Batch Loss = 12.069520, Accuracy = 0.997000038624\n",
      "Performance on test set: Training epochs #11560000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11580000: Batch Loss = 55.017094, Accuracy = 0.993000030518\n",
      "Performance on test set: Training epochs #11580000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11600000: Batch Loss = 22.668129, Accuracy = 0.997000098228\n",
      "Performance on test set: Training epochs #11600000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11620000: Batch Loss = 443.550385, Accuracy = 0.949000000954\n",
      "Performance on test set: Training epochs #11620000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11640000: Batch Loss = 17.609592, Accuracy = 0.995999932289\n",
      "Performance on test set: Training epochs #11640000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11660000: Batch Loss = 46.763981, Accuracy = 0.993000030518\n",
      "Performance on test set: Training epochs #11660000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11680000: Batch Loss = 0.860551, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #11680000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11700000: Batch Loss = 673.304199, Accuracy = 0.942000031471\n",
      "Performance on test set: Training epochs #11700000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11720000: Batch Loss = 23.455849, Accuracy = 0.994999945164\n",
      "Performance on test set: Training epochs #11720000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11740000: Batch Loss = 76.224701, Accuracy = 0.991000056267\n",
      "Performance on test set: Training epochs #11740000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11760000: Batch Loss = 9.401016, Accuracy = 0.998000144958\n",
      "Performance on test set: Training epochs #11760000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11780000: Batch Loss = 516.165649, Accuracy = 0.952000141144\n",
      "Performance on test set: Training epochs #11780000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11800000: Batch Loss = 35.967049, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #11800000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11820000: Batch Loss = 61.822464, Accuracy = 0.991999983788\n",
      "Performance on test set: Training epochs #11820000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #11840000: Batch Loss = 10.122710, Accuracy = 0.997000038624\n",
      "Performance on test set: Training epochs #11840000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #11860000: Batch Loss = 387.342712, Accuracy = 0.964000105858\n",
      "Performance on test set: Training epochs #11860000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11880000: Batch Loss = 42.240662, Accuracy = 0.995999932289\n",
      "Performance on test set: Training epochs #11880000, Batch Loss = 0.0, Accuracy = 1.00000011921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #11900000: Batch Loss = 46.667000, Accuracy = 0.995000064373\n",
      "Performance on test set: Training epochs #11900000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11920000: Batch Loss = 26.181702, Accuracy = 0.996000111103\n",
      "Performance on test set: Training epochs #11920000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11940000: Batch Loss = 417.403992, Accuracy = 0.95400005579\n",
      "Performance on test set: Training epochs #11940000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11960000: Batch Loss = 29.511028, Accuracy = 0.988999962807\n",
      "Performance on test set: Training epochs #11960000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #11980000: Batch Loss = 9.261507, Accuracy = 0.997000098228\n",
      "Performance on test set: Training epochs #11980000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12000000: Batch Loss = 8.230227, Accuracy = 0.998000144958\n",
      "Performance on test set: Training epochs #12000000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12020000: Batch Loss = 385.484436, Accuracy = 0.966000080109\n",
      "Performance on test set: Training epochs #12020000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12040000: Batch Loss = 36.758972, Accuracy = 0.991999983788\n",
      "Performance on test set: Training epochs #12040000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12060000: Batch Loss = 34.849380, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #12060000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12080000: Batch Loss = 10.323154, Accuracy = 0.998000025749\n",
      "Performance on test set: Training epochs #12080000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12100000: Batch Loss = 378.484375, Accuracy = 0.951000094414\n",
      "Performance on test set: Training epochs #12100000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12120000: Batch Loss = 70.841110, Accuracy = 0.989000082016\n",
      "Performance on test set: Training epochs #12120000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12140000: Batch Loss = 15.618750, Accuracy = 0.994000077248\n",
      "Performance on test set: Training epochs #12140000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12160000: Batch Loss = 2.821832, Accuracy = 0.996999979019\n",
      "Performance on test set: Training epochs #12160000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12180000: Batch Loss = 294.134216, Accuracy = 0.961000025272\n",
      "Performance on test set: Training epochs #12180000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12200000: Batch Loss = 12.776511, Accuracy = 0.994000077248\n",
      "Performance on test set: Training epochs #12200000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #12220000: Batch Loss = 118.404922, Accuracy = 0.990000009537\n",
      "Performance on test set: Training epochs #12220000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12240000: Batch Loss = 3.983408, Accuracy = 0.998000085354\n",
      "Performance on test set: Training epochs #12240000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12260000: Batch Loss = 616.737793, Accuracy = 0.952000081539\n",
      "Performance on test set: Training epochs #12260000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12280000: Batch Loss = 46.410442, Accuracy = 0.990000069141\n",
      "Performance on test set: Training epochs #12280000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12300000: Batch Loss = 7.365289, Accuracy = 0.998000025749\n",
      "Performance on test set: Training epochs #12300000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12320000: Batch Loss = 4.630392, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #12320000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #12340000: Batch Loss = 352.589630, Accuracy = 0.95500010252\n",
      "Performance on test set: Training epochs #12340000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #12360000: Batch Loss = 33.459030, Accuracy = 0.993000030518\n",
      "Performance on test set: Training epochs #12360000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12380000: Batch Loss = 21.031084, Accuracy = 0.997000038624\n",
      "Performance on test set: Training epochs #12380000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12400000: Batch Loss = 3.783335, Accuracy = 0.998000025749\n",
      "Performance on test set: Training epochs #12400000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12420000: Batch Loss = 358.539368, Accuracy = 0.963999986649\n",
      "Performance on test set: Training epochs #12420000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12440000: Batch Loss = 19.488060, Accuracy = 0.993000030518\n",
      "Performance on test set: Training epochs #12440000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12460000: Batch Loss = 49.855125, Accuracy = 0.993000030518\n",
      "Performance on test set: Training epochs #12460000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12480000: Batch Loss = 12.884531, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #12480000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12500000: Batch Loss = 468.888000, Accuracy = 0.944000005722\n",
      "Performance on test set: Training epochs #12500000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12520000: Batch Loss = 8.820591, Accuracy = 0.998000085354\n",
      "Performance on test set: Training epochs #12520000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12540000: Batch Loss = 17.613504, Accuracy = 0.994000077248\n",
      "Performance on test set: Training epochs #12540000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12560000: Batch Loss = 26.578806, Accuracy = 0.996999979019\n",
      "Performance on test set: Training epochs #12560000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12580000: Batch Loss = 380.633911, Accuracy = 0.953000068665\n",
      "Performance on test set: Training epochs #12580000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12600000: Batch Loss = 28.767956, Accuracy = 0.991999983788\n",
      "Performance on test set: Training epochs #12600000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12620000: Batch Loss = 24.777498, Accuracy = 0.996000051498\n",
      "Performance on test set: Training epochs #12620000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12640000: Batch Loss = 0.943472, Accuracy = 0.998000025749\n",
      "Performance on test set: Training epochs #12640000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12660000: Batch Loss = 329.513062, Accuracy = 0.951000094414\n",
      "Performance on test set: Training epochs #12660000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12680000: Batch Loss = 19.446573, Accuracy = 0.990999937057\n",
      "Performance on test set: Training epochs #12680000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12700000: Batch Loss = 48.385708, Accuracy = 0.990999996662\n",
      "Performance on test set: Training epochs #12700000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12720000: Batch Loss = 2.226370, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #12720000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12740000: Batch Loss = 357.813904, Accuracy = 0.948000073433\n",
      "Performance on test set: Training epochs #12740000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12760000: Batch Loss = 24.303387, Accuracy = 0.994000017643\n",
      "Performance on test set: Training epochs #12760000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12780000: Batch Loss = 26.455774, Accuracy = 0.994000077248\n",
      "Performance on test set: Training epochs #12780000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12800000: Batch Loss = 1.382893, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #12800000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12820000: Batch Loss = 431.113220, Accuracy = 0.95400005579\n",
      "Performance on test set: Training epochs #12820000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #12840000: Batch Loss = 16.934502, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #12840000, Batch Loss = 0.0, Accuracy = 1.00000011921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #12860000: Batch Loss = 40.216148, Accuracy = 0.992000043392\n",
      "Performance on test set: Training epochs #12860000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12880000: Batch Loss = 8.618397, Accuracy = 0.997000098228\n",
      "Performance on test set: Training epochs #12880000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12900000: Batch Loss = 258.057098, Accuracy = 0.961000025272\n",
      "Performance on test set: Training epochs #12900000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12920000: Batch Loss = 12.618738, Accuracy = 0.996000051498\n",
      "Performance on test set: Training epochs #12920000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12940000: Batch Loss = 22.436447, Accuracy = 0.998000085354\n",
      "Performance on test set: Training epochs #12940000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12960000: Batch Loss = 4.471009, Accuracy = 0.996999979019\n",
      "Performance on test set: Training epochs #12960000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #12980000: Batch Loss = 382.397675, Accuracy = 0.958000063896\n",
      "Performance on test set: Training epochs #12980000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13000000: Batch Loss = 29.312794, Accuracy = 0.992000043392\n",
      "Performance on test set: Training epochs #13000000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13020000: Batch Loss = 13.915343, Accuracy = 0.995000064373\n",
      "Performance on test set: Training epochs #13020000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13040000: Batch Loss = 11.631630, Accuracy = 0.997000098228\n",
      "Performance on test set: Training epochs #13040000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13060000: Batch Loss = 315.374146, Accuracy = 0.958000004292\n",
      "Performance on test set: Training epochs #13060000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13080000: Batch Loss = 11.519755, Accuracy = 0.998000085354\n",
      "Performance on test set: Training epochs #13080000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13100000: Batch Loss = 19.646873, Accuracy = 0.995000064373\n",
      "Performance on test set: Training epochs #13100000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13120000: Batch Loss = 3.294375, Accuracy = 0.998000144958\n",
      "Performance on test set: Training epochs #13120000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13140000: Batch Loss = 160.943741, Accuracy = 0.96899998188\n",
      "Performance on test set: Training epochs #13140000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13160000: Batch Loss = 8.043177, Accuracy = 0.996999979019\n",
      "Performance on test set: Training epochs #13160000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13180000: Batch Loss = 9.454551, Accuracy = 0.998000025749\n",
      "Performance on test set: Training epochs #13180000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13200000: Batch Loss = 7.974545, Accuracy = 0.996999979019\n",
      "Performance on test set: Training epochs #13200000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13220000: Batch Loss = 287.087524, Accuracy = 0.957000076771\n",
      "Performance on test set: Training epochs #13220000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13240000: Batch Loss = 26.711082, Accuracy = 0.995000064373\n",
      "Performance on test set: Training epochs #13240000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13260000: Batch Loss = 25.651625, Accuracy = 0.997000098228\n",
      "Performance on test set: Training epochs #13260000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13280000: Batch Loss = 6.070706, Accuracy = 0.996999979019\n",
      "Performance on test set: Training epochs #13280000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #13300000: Batch Loss = 195.147141, Accuracy = 0.961000025272\n",
      "Performance on test set: Training epochs #13300000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13320000: Batch Loss = 9.210443, Accuracy = 0.996000051498\n",
      "Performance on test set: Training epochs #13320000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13340000: Batch Loss = 61.423054, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #13340000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13360000: Batch Loss = 1.739667, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #13360000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #13380000: Batch Loss = 274.125214, Accuracy = 0.955000042915\n",
      "Performance on test set: Training epochs #13380000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13400000: Batch Loss = 25.295486, Accuracy = 0.991999983788\n",
      "Performance on test set: Training epochs #13400000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13420000: Batch Loss = 5.274796, Accuracy = 0.997000098228\n",
      "Performance on test set: Training epochs #13420000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13440000: Batch Loss = 13.810953, Accuracy = 0.997000098228\n",
      "Performance on test set: Training epochs #13440000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13460000: Batch Loss = 237.539642, Accuracy = 0.949000060558\n",
      "Performance on test set: Training epochs #13460000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13480000: Batch Loss = 28.089563, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #13480000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13500000: Batch Loss = 29.359192, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #13500000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #13520000: Batch Loss = 0.000000, Accuracy = 1.00000011921\n",
      "Performance on test set: Training epochs #13520000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13540000: Batch Loss = 232.812836, Accuracy = 0.957000017166\n",
      "Performance on test set: Training epochs #13540000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13560000: Batch Loss = 1.349267, Accuracy = 0.997000038624\n",
      "Performance on test set: Training epochs #13560000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13580000: Batch Loss = 35.545914, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #13580000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13600000: Batch Loss = 4.692637, Accuracy = 0.996999979019\n",
      "Performance on test set: Training epochs #13600000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13620000: Batch Loss = 154.137344, Accuracy = 0.961000084877\n",
      "Performance on test set: Training epochs #13620000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13640000: Batch Loss = 23.176376, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #13640000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13660000: Batch Loss = 47.775890, Accuracy = 0.994000077248\n",
      "Performance on test set: Training epochs #13660000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13680000: Batch Loss = 0.000000, Accuracy = 1.00000011921\n",
      "Performance on test set: Training epochs #13680000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13700000: Batch Loss = 334.460999, Accuracy = 0.958000063896\n",
      "Performance on test set: Training epochs #13700000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13720000: Batch Loss = 30.823061, Accuracy = 0.996000051498\n",
      "Performance on test set: Training epochs #13720000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #13740000: Batch Loss = 18.411098, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #13740000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13760000: Batch Loss = 6.733152, Accuracy = 0.998000085354\n",
      "Performance on test set: Training epochs #13760000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #13780000: Batch Loss = 284.769104, Accuracy = 0.953999996185\n",
      "Performance on test set: Training epochs #13780000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13800000: Batch Loss = 9.972094, Accuracy = 0.996999979019\n",
      "Performance on test set: Training epochs #13800000, Batch Loss = 0.0, Accuracy = 1.00000011921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #13820000: Batch Loss = 5.112835, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #13820000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13840000: Batch Loss = 10.705649, Accuracy = 0.997000038624\n",
      "Performance on test set: Training epochs #13840000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #13860000: Batch Loss = 288.514801, Accuracy = 0.953999996185\n",
      "Performance on test set: Training epochs #13860000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13880000: Batch Loss = 8.666089, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #13880000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13900000: Batch Loss = 14.886482, Accuracy = 0.996999979019\n",
      "Performance on test set: Training epochs #13900000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13920000: Batch Loss = 0.254562, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #13920000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13940000: Batch Loss = 236.980713, Accuracy = 0.964000046253\n",
      "Performance on test set: Training epochs #13940000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13960000: Batch Loss = 9.616605, Accuracy = 0.997000098228\n",
      "Performance on test set: Training epochs #13960000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #13980000: Batch Loss = 3.437328, Accuracy = 0.998000025749\n",
      "Performance on test set: Training epochs #13980000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14000000: Batch Loss = 1.391925, Accuracy = 0.998000025749\n",
      "Performance on test set: Training epochs #14000000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14020000: Batch Loss = 228.614777, Accuracy = 0.952000081539\n",
      "Performance on test set: Training epochs #14020000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14040000: Batch Loss = 0.000000, Accuracy = 1.00000011921\n",
      "Performance on test set: Training epochs #14040000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14060000: Batch Loss = 0.000000, Accuracy = 1.00000011921\n",
      "Performance on test set: Training epochs #14060000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #14080000: Batch Loss = 1.885475, Accuracy = 0.998000025749\n",
      "Performance on test set: Training epochs #14080000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14100000: Batch Loss = 130.278122, Accuracy = 0.968000054359\n",
      "Performance on test set: Training epochs #14100000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14120000: Batch Loss = 32.511986, Accuracy = 0.994000077248\n",
      "Performance on test set: Training epochs #14120000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14140000: Batch Loss = 10.705761, Accuracy = 0.996999979019\n",
      "Performance on test set: Training epochs #14140000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14160000: Batch Loss = 0.000000, Accuracy = 1.00000011921\n",
      "Performance on test set: Training epochs #14160000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #14180000: Batch Loss = 186.560684, Accuracy = 0.962999939919\n",
      "Performance on test set: Training epochs #14180000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14200000: Batch Loss = 10.556623, Accuracy = 0.998000025749\n",
      "Performance on test set: Training epochs #14200000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14220000: Batch Loss = 9.242021, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #14220000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14240000: Batch Loss = 8.300406, Accuracy = 0.997000098228\n",
      "Performance on test set: Training epochs #14240000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14260000: Batch Loss = 200.077591, Accuracy = 0.958000063896\n",
      "Performance on test set: Training epochs #14260000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14280000: Batch Loss = 9.804788, Accuracy = 0.995999991894\n",
      "Performance on test set: Training epochs #14280000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14300000: Batch Loss = 0.000000, Accuracy = 1.00000011921\n",
      "Performance on test set: Training epochs #14300000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14320000: Batch Loss = 4.267912, Accuracy = 0.998000085354\n",
      "Performance on test set: Training epochs #14320000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14340000: Batch Loss = 80.363571, Accuracy = 0.977000057697\n",
      "Performance on test set: Training epochs #14340000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14360000: Batch Loss = 2.961961, Accuracy = 0.998000085354\n",
      "Performance on test set: Training epochs #14360000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14380000: Batch Loss = 8.034939, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #14380000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14400000: Batch Loss = 16.856392, Accuracy = 0.996999979019\n",
      "Performance on test set: Training epochs #14400000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14420000: Batch Loss = 223.696014, Accuracy = 0.977999985218\n",
      "Performance on test set: Training epochs #14420000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14440000: Batch Loss = 26.105988, Accuracy = 0.993000030518\n",
      "Performance on test set: Training epochs #14440000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #14460000: Batch Loss = 0.591828, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #14460000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14480000: Batch Loss = 0.000000, Accuracy = 1.00000011921\n",
      "Performance on test set: Training epochs #14480000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14500000: Batch Loss = 93.610832, Accuracy = 0.963000059128\n",
      "Performance on test set: Training epochs #14500000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14520000: Batch Loss = 4.890996, Accuracy = 0.998000025749\n",
      "Performance on test set: Training epochs #14520000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14540000: Batch Loss = 0.237489, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #14540000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14560000: Batch Loss = 4.183871, Accuracy = 0.998000025749\n",
      "Performance on test set: Training epochs #14560000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14580000: Batch Loss = 216.458221, Accuracy = 0.971000015736\n",
      "Performance on test set: Training epochs #14580000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14600000: Batch Loss = 14.528391, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #14600000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14620000: Batch Loss = 20.848185, Accuracy = 0.994000077248\n",
      "Performance on test set: Training epochs #14620000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14640000: Batch Loss = 2.081932, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #14640000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14660000: Batch Loss = 113.698807, Accuracy = 0.965000092983\n",
      "Performance on test set: Training epochs #14660000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14680000: Batch Loss = 5.396962, Accuracy = 0.998000025749\n",
      "Performance on test set: Training epochs #14680000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14700000: Batch Loss = 1.039810, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #14700000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14720000: Batch Loss = 0.000000, Accuracy = 1.00000011921\n",
      "Performance on test set: Training epochs #14720000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14740000: Batch Loss = 112.456963, Accuracy = 0.971000015736\n",
      "Performance on test set: Training epochs #14740000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14760000: Batch Loss = 3.270763, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #14760000, Batch Loss = 0.0, Accuracy = 1.00000011921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #14780000: Batch Loss = 64.630463, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #14780000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14800000: Batch Loss = 7.640146, Accuracy = 0.995000064373\n",
      "Performance on test set: Training epochs #14800000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14820000: Batch Loss = 184.306030, Accuracy = 0.97100007534\n",
      "Performance on test set: Training epochs #14820000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14840000: Batch Loss = 23.254442, Accuracy = 0.993000030518\n",
      "Performance on test set: Training epochs #14840000, Batch Loss = 0.714161157608, Accuracy = 0.996999979019\n",
      "Training epochs #14860000: Batch Loss = 1.171916, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #14860000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14880000: Batch Loss = 2.297060, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #14880000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14900000: Batch Loss = 108.314240, Accuracy = 0.972000002861\n",
      "Performance on test set: Training epochs #14900000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14920000: Batch Loss = 5.002691, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #14920000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #14940000: Batch Loss = 6.373647, Accuracy = 0.997000098228\n",
      "Performance on test set: Training epochs #14940000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14960000: Batch Loss = 0.425717, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #14960000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #14980000: Batch Loss = 85.686913, Accuracy = 0.969000041485\n",
      "Performance on test set: Training epochs #14980000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #15000000: Batch Loss = 5.151811, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #15000000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15020000: Batch Loss = 3.747175, Accuracy = 0.997000098228\n",
      "Performance on test set: Training epochs #15020000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15040000: Batch Loss = 5.200768, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #15040000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15060000: Batch Loss = 55.665115, Accuracy = 0.97000002861\n",
      "Performance on test set: Training epochs #15060000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15080000: Batch Loss = 2.331744, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #15080000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15100000: Batch Loss = 33.978554, Accuracy = 0.996000051498\n",
      "Performance on test set: Training epochs #15100000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #15120000: Batch Loss = 1.118145, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #15120000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15140000: Batch Loss = 78.304848, Accuracy = 0.97000002861\n",
      "Performance on test set: Training epochs #15140000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15160000: Batch Loss = 6.746675, Accuracy = 0.997000098228\n",
      "Performance on test set: Training epochs #15160000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15180000: Batch Loss = 1.690104, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #15180000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15200000: Batch Loss = 0.000000, Accuracy = 1.00000011921\n",
      "Performance on test set: Training epochs #15200000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15220000: Batch Loss = 96.238861, Accuracy = 0.975000023842\n",
      "Performance on test set: Training epochs #15220000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15240000: Batch Loss = 9.166231, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #15240000, Batch Loss = 2.44544267654, Accuracy = 0.996999979019\n",
      "Training epochs #15260000: Batch Loss = 1.141452, Accuracy = 0.998000085354\n",
      "Performance on test set: Training epochs #15260000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15280000: Batch Loss = 0.689988, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #15280000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15300000: Batch Loss = 80.786362, Accuracy = 0.96899998188\n",
      "Performance on test set: Training epochs #15300000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #15320000: Batch Loss = 12.888138, Accuracy = 0.996000051498\n",
      "Performance on test set: Training epochs #15320000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15340000: Batch Loss = 12.050455, Accuracy = 0.995999991894\n",
      "Performance on test set: Training epochs #15340000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15360000: Batch Loss = 0.000000, Accuracy = 1.00000011921\n",
      "Performance on test set: Training epochs #15360000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15380000: Batch Loss = 125.014725, Accuracy = 0.965000033379\n",
      "Performance on test set: Training epochs #15380000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15400000: Batch Loss = 13.393550, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #15400000, Batch Loss = 1.71360290051, Accuracy = 0.996999979019\n",
      "Training epochs #15420000: Batch Loss = 8.036028, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #15420000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15440000: Batch Loss = 0.000000, Accuracy = 1.00000011921\n",
      "Performance on test set: Training epochs #15440000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15460000: Batch Loss = 72.637215, Accuracy = 0.972000062466\n",
      "Performance on test set: Training epochs #15460000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15480000: Batch Loss = 9.620193, Accuracy = 0.995000004768\n",
      "Performance on test set: Training epochs #15480000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15500000: Batch Loss = 3.574605, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #15500000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15520000: Batch Loss = 0.000000, Accuracy = 1.00000011921\n",
      "Performance on test set: Training epochs #15520000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15540000: Batch Loss = 103.708572, Accuracy = 0.976000010967\n",
      "Performance on test set: Training epochs #15540000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15560000: Batch Loss = 4.901824, Accuracy = 0.996000051498\n",
      "Performance on test set: Training epochs #15560000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15580000: Batch Loss = 0.027142, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #15580000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15600000: Batch Loss = 0.000000, Accuracy = 1.00000011921\n",
      "Performance on test set: Training epochs #15600000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15620000: Batch Loss = 63.216759, Accuracy = 0.979000031948\n",
      "Performance on test set: Training epochs #15620000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15640000: Batch Loss = 14.043868, Accuracy = 0.996000051498\n",
      "Performance on test set: Training epochs #15640000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15660000: Batch Loss = 5.615612, Accuracy = 0.995999932289\n",
      "Performance on test set: Training epochs #15660000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15680000: Batch Loss = 7.260723, Accuracy = 0.997000098228\n",
      "Performance on test set: Training epochs #15680000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15700000: Batch Loss = 156.054550, Accuracy = 0.967000067234\n",
      "Performance on test set: Training epochs #15700000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15720000: Batch Loss = 1.332044, Accuracy = 0.998000085354\n",
      "Performance on test set: Training epochs #15720000, Batch Loss = 0.0, Accuracy = 1.00000011921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epochs #15740000: Batch Loss = 10.263264, Accuracy = 0.996000051498\n",
      "Performance on test set: Training epochs #15740000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15760000: Batch Loss = 0.031947, Accuracy = 0.999000072479\n",
      "Performance on test set: Training epochs #15760000, Batch Loss = 0.0, Accuracy = 1.0\n",
      "Training epochs #15780000: Batch Loss = 118.910065, Accuracy = 0.960999965668\n",
      "Performance on test set: Training epochs #15780000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15800000: Batch Loss = 4.583143, Accuracy = 0.998000085354\n",
      "Performance on test set: Training epochs #15800000, Batch Loss = 2.04644608498, Accuracy = 0.996999979019\n",
      "Training epochs #15820000: Batch Loss = 3.054174, Accuracy = 0.998000025749\n",
      "Performance on test set: Training epochs #15820000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15840000: Batch Loss = 0.717600, Accuracy = 0.997000098228\n",
      "Performance on test set: Training epochs #15840000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15860000: Batch Loss = 57.131939, Accuracy = 0.969000041485\n",
      "Performance on test set: Training epochs #15860000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15880000: Batch Loss = 3.508784, Accuracy = 0.997000038624\n",
      "Performance on test set: Training epochs #15880000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15900000: Batch Loss = 5.718097, Accuracy = 0.996000051498\n",
      "Performance on test set: Training epochs #15900000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15920000: Batch Loss = 0.000000, Accuracy = 1.00000011921\n",
      "Performance on test set: Training epochs #15920000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15940000: Batch Loss = 69.832550, Accuracy = 0.969999969006\n",
      "Performance on test set: Training epochs #15940000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15960000: Batch Loss = 3.536762, Accuracy = 0.995000064373\n",
      "Performance on test set: Training epochs #15960000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #15980000: Batch Loss = 2.033963, Accuracy = 0.998000025749\n",
      "Performance on test set: Training epochs #15980000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Training epochs #16000000: Batch Loss = 5.709377, Accuracy = 0.998000025749\n",
      "Performance on test set: Training epochs #16000000, Batch Loss = 0.0, Accuracy = 1.00000011921\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "def extract_batch_size(_train, step, batch_size):   \n",
    "    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data.    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index] \n",
    "    return batch_s\n",
    "\n",
    "\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "test_predictions = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size <= training_iters:\n",
    "        batch_xs = extract_batch_size(train_x, step, batch_size)\n",
    "        batch_ys = extract_batch_size(train_y, step, batch_size)\n",
    "\n",
    "        # Fit training using batch data\n",
    "        _, loss, acc = sess.run([optimizer, cost, accuracy],\n",
    "                       feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})\n",
    "\n",
    "        train_losses.append(loss)\n",
    "        train_accuracies.append(acc)  \n",
    "        \n",
    "        \n",
    "        batch_xt = extract_batch_size(test_x, step, batch_size)\n",
    "        batch_yt = extract_batch_size(test_y, step, batch_size)\n",
    "    \n",
    "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "        test_predict, test_loss, test_acc = sess.run([pred, cost, accuracy], \n",
    "                                            feed_dict={x: batch_xt, y: batch_yt, keep_prob: 1.})\n",
    "        \n",
    "        test_predictions.append(np.argmax(test_predict, axis=1))\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        # Evaluate network only at some steps for faster training: \n",
    "        if (step*batch_size % display_step == 0) or (step == 1) \\\n",
    "            or (step * batch_size > training_iters):\n",
    "\n",
    "            print(\"Training epochs #\" + str(step*batch_size) + \\\n",
    "                  \": Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "                  \", Accuracy = {}\".format(acc))\n",
    "        \n",
    "            print(\"Performance on test set: \" + \"Training epochs #\" + str(step*batch_size) +\\\n",
    "                  \", Batch Loss = {}\".format(test_loss) + \", Accuracy = {}\".format(test_acc))\n",
    "        step += 1\n",
    "\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAHwCAYAAADATlvnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f096b5f5e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f08fc529290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#font = {'family': 'Bitstream Vera Sans', 'weight': 'bold', 'size': 12}\n",
    "#matplotlib.rc('font', **font)\n",
    "width = 12\n",
    "height = 8\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "indep_train_axis = np.array(range(batch_size, \n",
    "                   (len(train_losses)+1)*batch_size, batch_size))\n",
    "plt.plot(indep_train_axis, np.array(train_losses), \"b--\", label=\"Train losses\")\n",
    "plt.plot(indep_train_axis, np.array(train_accuracies), \"g--\", label=\"Train accuracies\")\n",
    "\n",
    "indep_test_axis = np.append(np.array(range(batch_size, \n",
    "                  len(test_losses)*batch_size, batch_size)), training_iters)\n",
    "plt.plot(indep_test_axis, np.array(test_losses), \"b-\", label=\"Test losses\")\n",
    "plt.plot(indep_test_axis, np.array(test_accuracies), \"g-\", label=\"Test accuracies\")\n",
    "\n",
    "plt.title(\"Training session's progress over iterations\")\n",
    "plt.legend(loc='upper right', shadow=False)\n",
    "plt.ylabel('Training Progress (Loss or Accuracy values)')\n",
    "plt.xlabel('Training iteration')\n",
    "plt.savefig('result/lstm_loss_accuracy.png')\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Testing Accuracy: 100.000011921%\n",
      "\n",
      "Precision: 97.7713562889%\n",
      "Recall: 97.71%\n",
      "f1_score: 97.6318941994%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[    0     3     0     0     0     5     0    10     0     0     0     0\n",
      "      0]\n",
      " [    6   797    26     0    27    24    21     4     5     0     0     1\n",
      "      0]\n",
      " [    0     0   213     0     0     0     0     0     0     0     0     0\n",
      "      0]\n",
      " [    0     2     0     0     0     0     0     0     0     0     0     0\n",
      "      0]\n",
      " [    0     2     0     0  2865    10     0    11     0     0     0     3\n",
      "      0]\n",
      " [    0    26     1     0     2   319     0     0     0     0     0     0\n",
      "      0]\n",
      " [    0     7     2     0     0     0 14486     0     0     0     0     0\n",
      "      0]\n",
      " [    0    66    16     0    39    36     5   372     0     0     0     1\n",
      "      0]\n",
      " [    0     0     0     0     0     0     0     0   132     0     0     0\n",
      "      0]\n",
      " [    0     0     2     0     0     0     0     0     0     2     0     0\n",
      "      0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     8     0\n",
      "      0]\n",
      " [    0    24    20     0     1    39     1     0     0     0     0   348\n",
      "      0]\n",
      " [    0     0     0     0     0    10     0     0     0     0     0     0\n",
      "      0]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     security       0.00      0.00      0.00        18\n",
      "       others       0.86      0.87      0.87       911\n",
      "    processor       0.76      1.00      0.86       213\n",
      "         disk       0.00      0.00      0.00         2\n",
      "      network       0.98      0.99      0.98      2891\n",
      "         file       0.72      0.92      0.81       348\n",
      "     database       1.00      1.00      1.00     14495\n",
      "      service       0.94      0.70      0.80       535\n",
      "       memory       0.96      1.00      0.98       132\n",
      "communication       1.00      0.50      0.67         4\n",
      "       system       1.00      1.00      1.00         8\n",
      "       driver       0.99      0.80      0.89       433\n",
      "           io       0.00      0.00      0.00        10\n",
      "\n",
      "  avg / total       0.98      0.98      0.98     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbbf7b7d950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo4AAAJGCAYAAADVtQQ5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nubKF8pCTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbbee00ec10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Results\n",
    "predictions = []\n",
    "length = len(test_predictions)-20\n",
    "for i in range(20):\n",
    "    predictions.extend(test_predictions[length+i])\n",
    "\n",
    "true_y = np.argmax(test_y, axis=1)\n",
    "\n",
    "print(\"Best Testing Accuracy: {}%\".format(100*max(test_accuracies)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Precision: {}%\".format(100*metrics.precision_score(\n",
    "    true_y, predictions, average=\"weighted\")))\n",
    "print(\"Recall: {}%\".format(100*metrics.recall_score(true_y, \n",
    "    predictions, average=\"weighted\")))\n",
    "print(\"f1_score: {}%\".format(100*metrics.f1_score(true_y, \n",
    "    predictions, average=\"weighted\")))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix = metrics.confusion_matrix(true_y, predictions)\n",
    "print(confusion_matrix)\n",
    "\n",
    "# Plot: \n",
    "## cmap can be changed to many colors, (colormaps.Oranges,OrRd, etc)\n",
    "def plot_CM(cm, title=\"Normalized Confusion Matrix\", cmap=plt.cm.summer):\n",
    "    width = 12\n",
    "    height = 8\n",
    "    plt.figure(figsize=(width, height))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(fault_label))\n",
    "    plt.xticks(tick_marks, fault_label.values(), rotation=90)\n",
    "    plt.yticks(tick_marks, fault_label.values())\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.show()\n",
    "    \n",
    "print(metrics.classification_report(\n",
    "    true_y, predictions, target_names = list(labels.values())))\n",
    "\n",
    "cm = confusion_matrix\n",
    "\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:,np.newaxis]\n",
    "plt.figure()\n",
    "plot_CM(cm_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
